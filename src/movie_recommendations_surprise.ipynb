{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68b03fba-c3f4-4fdc-9edc-55fe2e80d595",
   "metadata": {},
   "source": [
    "# Project Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a926e3-7cf8-4fdf-b0a0-864448ea7c0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Actually, I used an SVD model, not a hybrid model! Need to update the above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7f305-657e-48aa-9349-91f2383b8329",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48983751-4020-4533-a86c-996c92f41336",
   "metadata": {},
   "source": [
    "This project utilizes the MovieLens dataset, a widely used dataset in the field of recommender systems, containing movie ratings and metadata.  The data was obtained at https://www.kaggle.com/datasets/parasharmanas/movie-recommendation-system/data.  There are two data files. The first contains the unique movie Id, movie title and a list of genres the movie falls into.  The second consists of user ratings with the user ID, movie ID, rating and timestap of when the review was made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a070b573-1667-4103-ac8f-5ea67560a229",
   "metadata": {},
   "source": [
    "# REVISED CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e55fa9-ea6b-4624-b967-95e63463dde2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# conda install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca23c10-083c-4062-9207-2ee69e8a45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Optimized Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73342e-fac0-41c2-81f3-10d1de876aef",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc906f-3063-46b0-bf07-2ab4fb18b768",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load and Merge Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38723aea-be05-4aa7-8fbe-687e7f59a025",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load and merge movies and ratings data with essential columns and a subset of ratings\n",
    "def load_and_merge_data(movies_file, ratings_file):\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file, usecols=['movieId', 'title', 'genres'])\n",
    "        ratings_df = pd.read_csv(ratings_file, usecols=['userId', 'movieId', 'rating'])\n",
    "\n",
    "        # Remove duplicates\n",
    "        movies_df.drop_duplicates(subset=['movieId'], inplace=True)\n",
    "        ratings_df.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "        # Handle missing values\n",
    "        movies_df.fillna({'genres': '(no genres listed)'}, inplace=True)\n",
    "        ratings_df.dropna(subset=['userId', 'movieId', 'rating'], inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        logging.info(\"Data successfully merged and cleaned.\")\n",
    "        return merged_df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: {e}. File not found. Check the file paths.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf41964c-cec5-4820-b5d0-35b47a86669e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE TO RUN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3922b5ec-4bfc-4ea6-932a-dccf92d09881",
   "metadata": {},
   "source": [
    "# Initial EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee229b73-6991-484c-8b8e-428ec1725f0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2f25df-fb26-40dd-8b55-cb83b7bef9e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c1640-0e1e-4523-b38f-5ae228bee043",
   "metadata": {},
   "source": [
    "Movies Dataframe Summary:\n",
    "- There are 9,742 unique movies.\n",
    "- The genres column has 951 unique genre combinations, with 'Drama' being the most frequent.  The number of genres does not appear resaonable, so will conduct further analysis on.\n",
    "- The movieId ranges from 1 to 193,609, indicating a broad and possibly sparse numbering system, as there are only 9,737 unique movie titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c6f778-c401-46d9-9632-33248d5a277f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf6e3a4-f142-44ed-a6df-949b83c76e37",
   "metadata": {},
   "source": [
    "There are no missing values in the movies dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033d4f1-4f14-45e8-a2b8-47a4af367af0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "movies_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95781305-b895-4db2-9f23-05b2b81b3ad3",
   "metadata": {},
   "source": [
    "There are no duplicate rows in the movies dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c4a466-e251-4bbb-b106-1cf77c20bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e455d-9782-422f-a6ae-a4991370fcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b15b52-b223-4c18-aca9-252c133f8e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "Ratings Dataframe Summary:\n",
    "- Contains 100,836 ratings.\n",
    "- userId ranges from 1 to 610, indicating 610 unique users.\n",
    "- Ratings range from 0.5 to 5.0, in increments of 0.5.\n",
    "- The average rating is approximately 3.50.\n",
    "- timestamp is an integer representing the time the user rating was made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee82d8f-cfae-478c-9cc2-80583db9ae58",
   "metadata": {},
   "source": [
    "I will now merge the two datasets which will allow us to see which user rated which movie, along with the movie's title and genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec0031b-3e80-4ce2-b3e0-92361039f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two df's on movieId\n",
    "df = movies_df.merge(ratings_df, on='movieId')\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c859a70-e587-466e-a7d5-98916b1c11f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338ac546-1995-4c7a-a79f-0b2b1e408e4c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8794930-fc89-401a-9ca7-2cad23c07dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff004f77-e1e6-4a00-bcbf-05a04d24f3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbaa193-8f61-4e92-95a4-1b6eb9c93b42",
   "metadata": {},
   "source": [
    "Combined Dataset Statistics:\n",
    "- The dataset contains 100,836 ratings for 9,719 unique movies.\n",
    "- The genres column has 951 unique genre combinations, with 'Comedy' being the most frequent.\n",
    "- Ratings range from 0.5 to 5.0, with an average of approximately 3.50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96420a0e-0c64-41b2-8b48-32f2d12e5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e350e2-251e-4c70-bbae-e9608f6a5b6c",
   "metadata": {},
   "source": [
    "There are no missing values in the combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29413cb-2b37-4f8a-a594-9d0a37a73cbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997b4cd-3811-420b-9026-05bdd1f6571e",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28882b5e-e549-4acd-bd50-0f719ff610de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ratings distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='rating', hue='rating', data=df, legend=False, palette='muted'\n",
    "             )\n",
    "plt.title('Distribution of Ratings')\n",
    "plt.xlabel('Rating')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f56c0bc-3c8e-4411-8c59-84b206d58622",
   "metadata": {},
   "source": [
    "The distribution of ratings shows that:\n",
    "- Ratings are discrete, in increments of 0.5.\n",
    "- The most common ratings are around 3.0 to 4.0, indicating a tendency towards higher ratings.\n",
    "- The extreme ratings (0.5 and 5.0) are less common, suggesting that users are generally moderate in their assessments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed3ed1a-c915-4f20-bd02-21c15b1b490b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796efe22-d33e-4e60-9737-8a4daba4606f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe644f6e-c373-4c39-bb99-d69d2b87bad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split genres by | and add as a list\n",
    "df['genres'] = df['genres'].apply(lambda x:x.split('|'))\n",
    "                                  \n",
    "df.head()\n",
    "\n",
    "# Break out genres included in list and determine count of each\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Explode the genres column to have separate row for each genre\n",
    "exploded_genres = df.explode('genres')\n",
    "\n",
    "# Count the occurrences of each genre\n",
    "genre_counts = exploded_genres['genres'].value_counts()\n",
    "genre_counts\n",
    "\n",
    "# Plot the genre frequencies\n",
    "\n",
    "g = sns.catplot(genre_counts, kind='bar', palette='muted', height=5, aspect=2)\n",
    "g.fig.suptitle('Frequency of Movie Genres')\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of Movies')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954c6508-ec59-4b1c-b215-ba78feb3be52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f365e4ec-b196-4062-bfa9-6c83d4b0d3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad22641-05cf-42ce-afd5-7934182ff38e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d7424-331d-4c95-9bc3-9a3769d6b8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8ccee9b-2e6e-41be-9d51-5142e937c81f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Bivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406b49cd-63a3-4cab-8000-815e1a3f0faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming 'ratings_df' is your DataFrame\n",
    "user_rating_counts = ratings_df['userId'].value_counts()\n",
    "movie_rating_counts = ratings_df['movieId'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 6))\n",
    "\n",
    "# Plot for number of ratings per user\n",
    "axes[0].hist(user_rating_counts, bins=30, edgecolor='black')\n",
    "axes[0].set_title('Number of Ratings per User')\n",
    "axes[0].set_xlabel('Number of Ratings')\n",
    "axes[0].set_ylabel('Number of Users')\n",
    "\n",
    "# Plot for number of ratings per movie\n",
    "axes[1].hist(movie_rating_counts, bins=30, edgecolor='black')\n",
    "axes[1].set_title('Number of Ratings per Movie')\n",
    "axes[1].set_xlabel('Number of Ratings')\n",
    "axes[1].set_ylabel('Number of Movies')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66d3f2-6978-45ab-92c1-99fc57b9876b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Average Rating per User\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate average rating per user\n",
    "user_avg_ratings = ratings_df.groupby('userId')['rating'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(user_avg_ratings, bins=30, kde=True)\n",
    "plt.title('Average Rating per User')\n",
    "plt.xlabel('Average Rating')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3228d46a-912e-4297-b496-a93ecdd92cc2",
   "metadata": {},
   "source": [
    "# Distribution of Ratings across genres\n",
    "\n",
    "\n",
    "## Need to add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c6467c-42a4-495d-bc46-90ca41560cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6d9baf-470d-4317-9c6a-d7a1f1449ea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a519e94-06ed-4dd4-821d-f63dd4b5f86f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4846e45-1bdf-4536-bc1f-0a6d2c68fcef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d3418-44f9-4670-9563-b36988d04275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ratings over Time\n",
    "ratings_df['datetime'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\n",
    "ratings_df['year'] = ratings_df['datetime'].dt.year\n",
    "ratings_df.groupby('year')['rating'].mean().plot(kind='line')\n",
    "plt.title('Average Ratings Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Average Rating')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe762a4-7ca3-4d16-9309-4d771c5e78ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of Ratings Over Time\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'timestamp' is a UNIX timestamp in 'ratings_df'\n",
    "ratings_df['date'] = pd.to_datetime(ratings_df['timestamp'], unit='s')\n",
    "ratings_df['year'] = ratings_df['date'].dt.year\n",
    "ratings_df['month'] = ratings_df['date'].dt.month\n",
    "\n",
    "# Pivot table to prepare data for heatmap\n",
    "rating_pivot = ratings_df.pivot_table(values='rating', index='month', columns='year', aggfunc='mean')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(rating_pivot, annot=True, fmt=\".1f\", cmap='coolwarm')\n",
    "plt.title('Average Monthly Ratings Over the Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Month')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12206bec-594e-41cb-a7f2-dac4b7a0eceb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992609f-e8f5-45df-9666-b195660f0bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527c4b60-1bb7-4fa3-accc-f5b1d1e5c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788ea9ee-b5da-4f51-9477-b39a86b2cc2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50fdfbd-482a-45c5-a660-80ac82cf95b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e1302f-48f9-4d22-8cf6-37d8df63da94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefa5e9f-9c8b-4091-a74c-f136bc4b3558",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce1e2ab-eb03-4d83-b39c-41efa6700290",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6c109-f027-4ee3-859d-f54989114545",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b05a526e-f4ff-4ee2-a376-2e3b817f80a3",
   "metadata": {},
   "source": [
    "## Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b58cef8-1f03-472e-baf8-60c1357cab85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preprocess data by adding one-hot encoding for genres\n",
    "def preprocess_data(df):\n",
    "    if 'genres' in df.columns:\n",
    "        df['genres'] = df['genres'].replace('', '(no genres listed)')\n",
    "        genres_dummies = df['genres'].str.get_dummies(sep='|')\n",
    "        df = pd.concat([df, genres_dummies], axis=1)\n",
    "        logging.info(\"Genre-based features added.\")\n",
    "    else:\n",
    "        logging.info(\"Genres column not available for processing.\")\n",
    "    return df\n",
    "\n",
    "# Add user-genre interaction features to the dataset\n",
    "def add_user_genre_features(df):\n",
    "    if 'userId' in df.columns:\n",
    "        # Select only genre columns for aggregation\n",
    "        genre_columns = [col for col in df.columns if col not in ('user_mean_', 'title', 'movieId', 'userId', 'rating') and df[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "        # Compute mean genre features per user\n",
    "        user_genre_means = df.groupby(['userId'])[genre_columns].mean()\n",
    "        user_genre_means.columns = [f'user_mean_{col}' for col in user_genre_means.columns]\n",
    "\n",
    "        # Merge the new features back into the main DataFrame\n",
    "        df = pd.merge(df, user_genre_means, on='userId', how='left')\n",
    "        logging.info(\"User-genre interaction features added.\")\n",
    "    else:\n",
    "        logging.info(\"UserId column not available for interaction features.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8f3d6-1cdf-4351-9c92-98af33b12570",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Additional EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfea2eb5-9278-4c92-8b0f-981bec306a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243d8ef7-ec8e-4f0e-83ae-19910d257f6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2bec0d-22a4-479b-a33c-6fe7e133a4bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48143b1-6cc0-4da1-b2a4-c6ff93c2c304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214d23a-b3ca-4d99-9c40-d044a80b316b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88c0e0-9d12-4d90-afdf-abee8e1c636d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f73d8f-703c-4033-bdae-d71ab7087000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import logging\n",
    "\n",
    "# Helper function for hybrid recommendations per user\n",
    "def hybrid_user_recommendation_worker(user_id, df, svd_model, movie_indices, distances, indices, weight_svd, weight_content, n):\n",
    "    user_recommendations = []\n",
    "\n",
    "    try:\n",
    "        for idx in movie_indices:\n",
    "            sim_scores = list(enumerate(distances[idx]))\n",
    "            sorted_indices = np.argsort([x[1] for x in sim_scores])[::-1][1:11]\n",
    "\n",
    "            movie_indices_list = [indices[idx][i] for i in sorted_indices if indices[idx][i] in df.index]\n",
    "\n",
    "            sim_movies = df['movieId'].iloc[movie_indices_list]\n",
    "            svd_recs = [svd_model.predict(user_id, iid) for iid in sim_movies]\n",
    "\n",
    "            recommendations = [\n",
    "                (\n",
    "                    iid,\n",
    "                    weight_svd * est.est + weight_content * distances[idx][movie_indices_list.index(iid)]\n",
    "                )\n",
    "                for iid, est in zip(sim_movies, svd_recs)\n",
    "                if iid in movie_indices_list\n",
    "            ]\n",
    "\n",
    "            recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            user_recommendations.extend(recommendations[:n])\n",
    "\n",
    "        user_recommendations = sorted(user_recommendations, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "        return [(user_id, rec[0], rec[1]) for rec in user_recommendations]\n",
    "\n",
    "    except ValueError as e:\n",
    "        logging.error(f\"Error in hybrid_user_recommendation_worker: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to calculate sparse cosine similarity\n",
    "def calculate_sparse_cosine_similarity(tfidf_matrix, n_neighbors=10):\n",
    "    logging.info(\"Calculating sparse cosine similarity...\")\n",
    "    nbrs = NearestNeighbors(n_neighbors=n_neighbors, metric='cosine', algorithm='brute').fit(tfidf_matrix)\n",
    "    distances, indices = nbrs.kneighbors(tfidf_matrix)\n",
    "    logging.info(\"Sparse cosine similarity calculated.\")\n",
    "    return distances, indices\n",
    "\n",
    "# Hybrid recommendation function combining SVD and content-based filtering\n",
    "def hybrid_recommendation_to_screen(df, svd_model, test_users, weight_svd=0.7, weight_content=0.3, n=5, batch_size=2000):\n",
    "    movie_indices = pd.Series(df.index, index=df['title']).drop_duplicates()\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=1000, stop_words='english')  # Increase vocabulary size to 1000\n",
    "    tfidf_matrix = tfidf.fit_transform(df['genres'].fillna(''))\n",
    "    distances, indices = calculate_sparse_cosine_similarity(tfidf_matrix, n_neighbors=10)\n",
    "\n",
    "    all_recommendations = []\n",
    "\n",
    "    logging.info(f\"Processing users in batches of {batch_size}...\")\n",
    "\n",
    "    # Process users in batches\n",
    "    for i in range(0, len(test_users), batch_size):\n",
    "        batch_users = list(test_users)[i:i + batch_size]\n",
    "\n",
    "        # Set n_jobs to -1 to use all processors\n",
    "        batch_recommendations = Parallel(n_jobs=-1)(\n",
    "            delayed(hybrid_user_recommendation_worker)(\n",
    "                user_id, df, svd_model, movie_indices, distances, indices, weight_svd, weight_content, n\n",
    "            )\n",
    "            for user_id in batch_users\n",
    "        )\n",
    "        all_recommendations.extend([item for sublist in batch_recommendations for item in sublist])\n",
    "\n",
    "    # Print recommendations to screen\n",
    "    for rec in all_recommendations:\n",
    "        print(f\"User: {rec[0]}, Movie: {rec[1]}, Score: {rec[2]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faf9c6a-b3b1-4e54-acb0-860874cf0745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "765c3213-5ec7-4d22-abc8-e62fe0f119d1",
   "metadata": {},
   "source": [
    "## Train and Evaluate SVD Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010c6f3-3372-41fc-97d4-4f15f063a75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Train and evaluate the SVD model using fixed parameters\n",
    "def train_and_evaluate_svd(df):\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "    trainset, testset = train_test_split(data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "    svd_params = {\n",
    "        'n_factors': [150, 200, 250],  # Increase factors for better training\n",
    "        'n_epochs': [30, 40, 50],   # Increase epochs\n",
    "        'lr_all': 0.005,\n",
    "        'reg_all': 0.1,\n",
    "        'biased': True,\n",
    "        'random_state': RANDOM_SEED  # Add random seed for reproducibility\n",
    "    }\n",
    "\n",
    "    model = SVD(**svd_params)\n",
    "    model.fit(trainset)\n",
    "\n",
    "    predictions = model.test(testset)\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    mae = accuracy.mae(predictions)\n",
    "\n",
    "    print(f\"Best Model (RMSE): RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "    print(f\"Best Parameters: {svd_params}\")\n",
    "\n",
    "    return model, testset\n",
    "    \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342df211-692c-407d-a183-b590fdb440df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "best_svd_model, svd_testset = train_and_evaluate_svd(augmented_ratings_df)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a4ab5a-c30d-4a5f-a27c-361b48f71cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6566b1-6df8-400b-92d4-64701776e92e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surprise import SVD, Dataset, Reader, accuracy\n",
    "from surprise.model_selection import train_test_split, RandomizedSearchCV\n",
    "import logging\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Train and evaluate the SVD model using randomized search\n",
    "def train_and_evaluate_svd(df, threshold=4.0, n=5):\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "    trainset, testset = train_test_split(data, test_size=0.25, random_state=RANDOM_SEED)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_factors': [150],  # Adjust for complexity\n",
    "        'n_epochs': [100],  # Adjust for learning duration\n",
    "        'lr_all': [0.01],\n",
    "        'reg_all': [0.1],\n",
    "        'biased': [True]\n",
    "    }\n",
    "\n",
    "    randomized_search = RandomizedSearchCV(\n",
    "        SVD,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        random_state=RANDOM_SEED,\n",
    "        measures=['rmse', 'mae'],\n",
    "        cv=3,\n",
    "        refit=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    randomized_search.fit(data)\n",
    "\n",
    "    best_params = randomized_search.best_params['rmse']\n",
    "    best_model = randomized_search.best_estimator['rmse']\n",
    "\n",
    "    predictions = best_model.test(testset)\n",
    "    rmse = accuracy.rmse(predictions)\n",
    "    mae = accuracy.mae(predictions)\n",
    "\n",
    "    logging.info(f\"Best Model (RMSE): RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "    logging.info(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    print(f\"Best Model (RMSE): RMSE={rmse:.4f}, MAE={mae:.4f}\")\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    return best_model, testset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf289ac7-c1b4-4014-ab59-4470257eeae2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a21d84-84e1-4577-a805-157ce909cf52",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90cc46-c6a3-4af1-be69-d855c6758fef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate recommendation metrics (precision, recall, and F1-score)\n",
    "def calculate_recommendation_metrics(predictions, threshold=4.0, n=5):\n",
    "    def get_top_n(predictions, n=5):\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est, true_r))\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "        return top_n\n",
    "\n",
    "    top_n = get_top_n(predictions, n)\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for iid, est_rating, true_rating in user_ratings:\n",
    "            y_true.append(1 if true_rating >= threshold else 0)\n",
    "            y_pred.append(1 if est_rating >= threshold else 0)\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb7424f-db5b-4d3c-9d54-a823cb0cc1a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e2054f-5f7b-4d3c-af3f-a04a0d379a21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f996d7-b171-4987-bb7f-e81e2d3bdffb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f788a11-2092-49bb-9ad8-8e68c6edbae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002e151c-63db-4a42-a730-a0c8fd8cb0e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install memory_profiler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e53bd7-be78-4f54-bebc-743da567b7ff",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OLD CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6ddf3d-14db-4e27-b69b-54cd1d063a0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, Reader, SVD, accuracy\n",
    "from surprise.model_selection import RandomizedSearchCV, cross_validate, KFold\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_squared_error, mean_absolute_error\n",
    "from collections import defaultdict\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Constants for easier configuration\n",
    "MAX_RATINGS = 50000\n",
    "TFIDF_MAX_FEATURES = 100\n",
    "TOP_N = 5\n",
    "THRESHOLD = 4.0\n",
    "MIN_SCORE = 4.0\n",
    "SVD_PARAM_GRID = {\n",
    "    'n_factors': [40, 50, 60],\n",
    "    'n_epochs': [30, 40, 50],\n",
    "    'lr_all': [0.005, 0.007, 0.01],\n",
    "    'reg_all': [0.1, 0.2]\n",
    "}\n",
    "SVD_FIXED_PARAMS = {'biased': [True]}\n",
    "K_VALUES = list(range(1, 26))\n",
    "\n",
    "# Function to load and merge data\n",
    "def load_and_merge_data(movies_file, ratings_file, max_ratings=MAX_RATINGS):\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file, usecols=['movieId', 'title', 'genres'])\n",
    "        ratings_df = pd.read_csv(ratings_file, usecols=['userId', 'movieId', 'rating']).head(max_ratings)\n",
    "\n",
    "        movies_df.drop_duplicates(subset=['movieId'], inplace=True)\n",
    "        ratings_df.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "        movies_df.fillna({'genres': '(no genres listed)'}, inplace=True)\n",
    "        ratings_df.dropna(subset=['userId', 'movieId', 'rating'], inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        logging.info(\"Data successfully merged and cleaned.\")\n",
    "        return merged_df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: {e}. File not found. Check the file paths.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None\n",
    "\n",
    "# Preprocess data by adding one-hot encoding for genres\n",
    "def preprocess_data(df):\n",
    "    if 'genres' in df.columns:\n",
    "        df['genres'] = df['genres'].replace('', '(no genres listed)')\n",
    "        genres_dummies = df['genres'].str.get_dummies(sep='|')\n",
    "        df = pd.concat([df, genres_dummies], axis=1)\n",
    "        logging.info(\"Genre-based features added.\")\n",
    "    else:\n",
    "        logging.error(\"Genres column not available for processing.\")\n",
    "    return df\n",
    "\n",
    "# Add user-genre interaction features to the dataset\n",
    "def add_user_genre_features(df):\n",
    "    if 'userId' in df.columns:\n",
    "        genre_columns = [col for col in df.columns if col not in ('user_mean_', 'title', 'movieId', 'userId', 'rating') and df[col].dtype in [np.float64, np.int64]]\n",
    "        \n",
    "        user_genre_means = df.groupby(['userId'])[genre_columns].mean()\n",
    "        user_genre_means.columns = [f'user_mean_{col}' for col in user_genre_means.columns]\n",
    "        \n",
    "        df = pd.merge(df, user_genre_means, on='userId', how='left')\n",
    "        logging.info(\"User-genre interaction features added.\")\n",
    "    else:\n",
    "        logging.error(\"UserId column not available for interaction features.\")\n",
    "    return df\n",
    "\n",
    "# Function to calculate recommendation metrics\n",
    "def calculate_recommendation_metrics(predictions, user_rated_movies, threshold=THRESHOLD, n=TOP_N):\n",
    "    def get_top_n(predictions, n=5):\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            if iid not in user_rated_movies.get(uid, []):\n",
    "                top_n[uid].append((iid, est, true_r))\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = [rating for rating in user_ratings if rating[1] >= threshold][:n]\n",
    "        return top_n\n",
    "\n",
    "    top_n = get_top_n(predictions, n)\n",
    "    y_true, y_pred = [], []\n",
    "\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        for iid, est_rating, true_rating in user_ratings:\n",
    "            y_true.append(1 if true_rating >= threshold else 0)\n",
    "            y_pred.append(1 if est_rating >= threshold else 0)\n",
    "\n",
    "    if not y_true or not y_pred:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "# Function to print evaluation metrics\n",
    "def print_evaluation_metrics(predictions, user_rated_movies, threshold=THRESHOLD, n=TOP_N):\n",
    "    precision, recall, f1 = calculate_recommendation_metrics(predictions, user_rated_movies, threshold, n)\n",
    "\n",
    "    # Filter out seen movies for RMSE and MAE\n",
    "    filtered_predictions = [\n",
    "        (uid, iid, true_r, est)\n",
    "        for uid, iid, true_r, est, _ in predictions\n",
    "        if iid not in user_rated_movies.get(uid, []) and est >= threshold\n",
    "    ]\n",
    "    if not filtered_predictions:\n",
    "        logging.warning(\"No unseen predictions available for evaluation metrics.\")\n",
    "        print(\"No unseen predictions available for evaluation metrics.\")\n",
    "        return\n",
    "\n",
    "    true_ratings = [true_r for _, _, true_r, _ in filtered_predictions]\n",
    "    estimated_ratings = [est_r for _, _, _, est_r in filtered_predictions]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "\n",
    "    logging.info(f\"RMSE: {rmse:.4f}\")\n",
    "    logging.info(f\"MAE: {mae:.4f}\")\n",
    "    logging.info(f\"Precision: {precision:.4f}\")\n",
    "    logging.info(f\"Recall: {recall:.4f}\")\n",
    "    logging.info(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Train and evaluate SVD model with cross-validation\n",
    "def train_and_evaluate_svd(df):\n",
    "    reader = Reader(rating_scale=(0.5, 5.0))\n",
    "    data = Dataset.load_from_df(df[['userId', 'movieId', 'rating']], reader)\n",
    "\n",
    "    param_grid = {\n",
    "        'n_factors': [40, 50, 60],\n",
    "        'n_epochs': [30, 40, 50],\n",
    "        'lr_all': [0.005, 0.007, 0.01],\n",
    "        'reg_all': [0.1, 0.2]\n",
    "    }\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        SVD,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=10,\n",
    "        measures=['rmse', 'mae'],\n",
    "        cv=5,  # 5-Fold Cross-Validation\n",
    "        refit=True,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    random_search.fit(data)\n",
    "\n",
    "    best_params = random_search.best_params['rmse']\n",
    "    best_model = random_search.best_estimator['rmse']\n",
    "\n",
    "    # Evaluate with cross-validation\n",
    "    kf = KFold(n_splits=5)\n",
    "    results = cross_validate(best_model, data, measures=['rmse', 'mae'], cv=kf, verbose=True)\n",
    "\n",
    "    logging.info(f\"Cross-Validation RMSE: {np.mean(results['test_rmse']):.4f}\")\n",
    "    logging.info(f\"Cross-Validation MAE: {np.mean(results['test_mae']):.4f}\")\n",
    "    logging.info(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    trainset = data.build_full_trainset()\n",
    "    best_model.fit(trainset)\n",
    "\n",
    "    testset = trainset.build_testset()\n",
    "    predictions = best_model.test(testset)\n",
    "\n",
    "    return best_model, predictions\n",
    "\n",
    "# Hybrid recommendation function combining SVD and content-based filtering\n",
    "def hybrid_user_recommendation_worker(user_id, df, svd_model, cosine_sim, movie_indices, weight_svd, weight_content, n, min_score):\n",
    "    svd_recs = [svd_model.predict(user_id, iid) for iid in df['movieId']]\n",
    "    \n",
    "    # Ensure unique recommendations per user\n",
    "    svd_recs = sorted(svd_recs, key=lambda x: x.est, reverse=True)\n",
    "    svd_recs = list({rec.iid: rec for rec in svd_recs if rec.est >= min_score}.values())\n",
    "\n",
    "    user_indices = df[df['userId'] == user_id].index\n",
    "    content_scores = []\n",
    "    for user_idx in user_indices:\n",
    "        sim_scores = list(enumerate(cosine_sim[user_idx]))\n",
    "        sim_scores_sorted = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "        content_scores.extend([(df['movieId'][idx], score) for idx, score in sim_scores_sorted[:n]])\n",
    "\n",
    "    recommendations = []\n",
    "    seen_movies = set(df[df['userId'] == user_id]['movieId'])  # Movies already rated by the user\n",
    "    for iid, est_rating in [(rec.iid, rec.est) for rec in svd_recs if rec.iid not in seen_movies]:\n",
    "        content_score = next((score for mid, score in content_scores if mid == iid), 0)\n",
    "        combined_score = weight_svd * est_rating + weight_content * content_score\n",
    "        recommendations.append((iid, combined_score))\n",
    "\n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [(user_id, rec[0], rec[1]) for rec in recommendations]\n",
    "\n",
    "# Hybrid recommendation function combining SVD and content-based filtering\n",
    "def hybrid_recommendation_to_screen(df, svd_model, test_users, weight_svd=0.7, weight_content=0.3, n=TOP_N, min_score=MIN_SCORE, batch_size=2000):\n",
    "    movie_indices = pd.Series(df.index, index=df['movieId']).drop_duplicates()\n",
    "\n",
    "    tfidf = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(df['genres'].fillna(''))\n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    all_recommendations = []\n",
    "\n",
    "    for i in range(0, len(test_users), batch_size):\n",
    "        batch_users = list(test_users)[i:i + batch_size]\n",
    "        batch_recommendations = Parallel(n_jobs=-1)(\n",
    "            delayed(hybrid_user_recommendation_worker)(\n",
    "                user_id, df, svd_model, cosine_sim, movie_indices, weight_svd, weight_content, n, min_score\n",
    "            )\n",
    "            for user_id in batch_users\n",
    "        )\n",
    "        all_recommendations.extend([item for sublist in batch_recommendations for item in sublist])\n",
    "\n",
    "    recommendations_df = pd.DataFrame(all_recommendations, columns=['UserID', 'MovieID', 'Score'])\n",
    "    \n",
    "    # Assign unique actual ratings per user\n",
    "    def assign_actual_rating(row):\n",
    "        actual_ratings = df[(df['userId'] == row['UserID']) & (df['movieId'] == row['MovieID'])]['rating'].values\n",
    "        return actual_ratings[0] if len(actual_ratings) > 0 else None\n",
    "\n",
    "    recommendations_df = recommendations_df.merge(df[['movieId', 'title']].drop_duplicates(), left_on='MovieID', right_on='movieId', how='left')\n",
    "    recommendations_df['Actual Rating'] = recommendations_df.apply(assign_actual_rating, axis=1)\n",
    "    recommendations_df.drop(columns=['movieId'], inplace=True)\n",
    "    return recommendations_df\n",
    "\n",
    "# Print top N recommendations for sample users\n",
    "def print_top_n_recommendations(recommendations_df, n=TOP_N):\n",
    "    unique_users = recommendations_df['UserID'].unique()\n",
    "    for user_id in unique_users[:5]:\n",
    "        user_recs = recommendations_df[recommendations_df['UserID'] == user_id]\n",
    "        print(f\"\\nTop {n} Recommendations for User {user_id}:\")\n",
    "        for _, row in user_recs.head(n).iterrows():\n",
    "            print(f\"  MovieID: {int(row['MovieID'])}, Title: {row['title']}, Recommended Score: {row['Score']:.2f}, Actual Rating: {row['Actual Rating']}\")\n",
    "\n",
    "# Plot RMSE, MAE, Precision, Recall, and F1-Score at different K-values\n",
    "def plot_metrics_at_k(svd_predictions, k_values, threshold=THRESHOLD):\n",
    "    def get_top_n(predictions, n):\n",
    "        \"\"\"Return top-N predictions for each user\"\"\"\n",
    "        top_n = defaultdict(list)\n",
    "        for uid, iid, true_r, est, _ in predictions:\n",
    "            top_n[uid].append((iid, est, true_r))\n",
    "        for uid, user_ratings in top_n.items():\n",
    "            user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "            top_n[uid] = user_ratings[:n]\n",
    "        return top_n\n",
    "\n",
    "    rmse_values, mae_values = [], []\n",
    "    precision_values, recall_values, f1_values = [], [], []\n",
    "\n",
    "    for k in k_values:\n",
    "        top_n_predictions = get_top_n(svd_predictions, k)\n",
    "\n",
    "        # Prepare y_true and y_pred lists\n",
    "        y_true, y_pred = [], []\n",
    "        for uid, user_ratings in top_n_predictions.items():\n",
    "            for _, est_rating, true_rating in user_ratings:\n",
    "                y_true.append(1 if true_rating >= threshold else 0)\n",
    "                y_pred.append(1 if est_rating >= threshold else 0)\n",
    "\n",
    "        # Calculate Precision, Recall, and F1-Score\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        precision_values.append(precision)\n",
    "        recall_values.append(recall)\n",
    "        f1_values.append(f1)\n",
    "\n",
    "        # Calculate RMSE and MAE using the original prediction results\n",
    "        top_actual_ratings = [true_r for _, _, true_r in sum(top_n_predictions.values(), [])]\n",
    "        top_estimated_ratings = [est_r for _, est_r, _ in sum(top_n_predictions.values(), [])]\n",
    "\n",
    "        rmse_values.append(np.sqrt(mean_squared_error(top_actual_ratings, top_estimated_ratings)))\n",
    "        mae_values.append(mean_absolute_error(top_actual_ratings, top_estimated_ratings))\n",
    "\n",
    "    # Plot RMSE and MAE\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "    ax1.plot(k_values, rmse_values, label='RMSE', color='tab:red')\n",
    "    ax1.plot(k_values, mae_values, label='MAE', color='tab:orange')\n",
    "    ax1.set_xlabel('K-Value')\n",
    "    ax1.set_ylabel('RMSE/MAE', color='tab:red')\n",
    "    ax1.legend(loc='upper left')\n",
    "    plt.title('RMSE and MAE at Different K-Values')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot Precision, Recall, and F1-Score\n",
    "    fig, ax2 = plt.subplots(figsize=(10, 6))\n",
    "    ax2.plot(k_values, precision_values, label='Precision', color='tab:blue')\n",
    "    ax2.plot(k_values, recall_values, label='Recall', color='tab:green')\n",
    "    ax2.plot(f1_values, label='F1-Score', color='tab:purple')\n",
    "    ax2.set_xlabel('K-Value')\n",
    "    ax2.set_ylabel('Precision/Recall/F1-Score', color='tab:blue')\n",
    "    ax2.legend(loc='upper right')\n",
    "    plt.title('Precision, Recall, and F1-Score at Different K-Values')\n",
    "    plt.show()\n",
    "\n",
    "# Example Usage\n",
    "movies_file = '../data/movies.csv'\n",
    "ratings_file = '../data/ratings.csv'\n",
    "\n",
    "augmented_ratings_df = load_and_merge_data(movies_file, ratings_file, max_ratings=MAX_RATINGS)\n",
    "\n",
    "if augmented_ratings_df is not None:\n",
    "    augmented_ratings_df = preprocess_data(augmented_ratings_df)\n",
    "    augmented_ratings_df = add_user_genre_features(augmented_ratings_df)\n",
    "\n",
    "    best_svd_model, svd_predictions = train_and_evaluate_svd(augmented_ratings_df)\n",
    "\n",
    "    # Create a dictionary of already rated movies per user\n",
    "    user_rated_movies = defaultdict(list)\n",
    "    for row in augmented_ratings_df.itertuples():\n",
    "        user_rated_movies[row.userId].append(row.movieId)\n",
    "\n",
    "    # Get prediction results and evaluate\n",
    "    print_evaluation_metrics(svd_predictions, user_rated_movies, threshold=THRESHOLD, n=TOP_N)\n",
    "\n",
    "    # Extract unique test users from predictions\n",
    "    test_users = {pred[0] for pred in svd_predictions}\n",
    "\n",
    "    # Generate hybrid recommendations and print them to screen\n",
    "    recommendations_df = hybrid_recommendation_to_screen(augmented_ratings_df, best_svd_model, test_users, n=TOP_N)\n",
    "\n",
    "    # Print top 5 recommendations for 5 sample users\n",
    "    print_top_n_recommendations(recommendations_df, n=TOP_N)\n",
    "\n",
    "    # Plot RMSE, MAE, Precision, Recall, and F1-Score at Different K-Values\n",
    "    plot_metrics_at_k(svd_predictions, K_VALUES)\n",
    "else:\n",
    "    logging.error(\"Data loading failed, model training not performed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a114-5d71-42a1-9b45-eccddd31ac1f",
   "metadata": {},
   "source": [
    "# Last code that run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e7aca-75ae-44e7-bb55-79a7169c6f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def load_and_merge_data(movies_file, ratings_file, max_ratings=None):\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file, usecols=['movieId', 'title', 'genres'])\n",
    "        ratings_df = pd.read_csv(ratings_file, usecols=['userId', 'movieId', 'rating']).head(max_ratings)\n",
    "\n",
    "        # Drop duplicates if any exist\n",
    "        movies_df.drop_duplicates(subset='movieId', inplace=True)\n",
    "        ratings_df.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "        # Perform merge\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "\n",
    "        # Create a unique key by combining userId and movieId\n",
    "        merged_df['uniqueId'] = merged_df['userId'].astype(str) + \"_\" + merged_df['movieId'].astype(str)\n",
    "\n",
    "        logging.info(\"Data loaded and merged successfully.\")\n",
    "        return merged_df, movies_df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: {e}. File not found. Check the file paths.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_release_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    if 'title' in df.columns:\n",
    "        df['release_year'] = df['title'].apply(extract_release_year)\n",
    "        df['release_year'].fillna(0, inplace=True)  # Fill NaN with 0 for release year\n",
    "        logging.info(\"Release year feature added.\")\n",
    "    else:\n",
    "        logging.error(\"Title column not available for processing.\")\n",
    "    \n",
    "    if 'genres' in df.columns:\n",
    "        # Split genres by | and add as a list\n",
    "        df['genres'] = df['genres'].apply(lambda x: x.split('|'))\n",
    "        \n",
    "        # Explode the genres column to have a separate row for each genre\n",
    "        exploded_genres = df.explode('genres')\n",
    "        \n",
    "        # One-hot encode the genres\n",
    "        genres_dummies = pd.get_dummies(exploded_genres['genres'], prefix='genre')\n",
    "        \n",
    "        # Concatenate the original DataFrame with the one-hot encoded genres\n",
    "        df = pd.concat([exploded_genres, genres_dummies], axis=1)\n",
    "        \n",
    "        # Drop the original genres column\n",
    "        df.drop(columns=['genres'], inplace=True)\n",
    "        \n",
    "        # Drop duplicate rows\n",
    "        df = df.drop_duplicates(subset=['userId', 'movieId', 'rating', 'title', 'uniqueId', 'release_year']).reset_index(drop=True)\n",
    "        \n",
    "        logging.info(\"Genre-based features added.\")\n",
    "    else:\n",
    "        logging.error(\"Genres column not available for processing.\")\n",
    "    \n",
    "    logging.info(f\"Data after preprocessing: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def add_user_genre_features(df):\n",
    "    if 'userId' in df.columns:\n",
    "        # Ensure only numeric columns are selected for aggregation\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        genre_columns = [col for col in numeric_cols if col.startswith('genre_')]\n",
    "        \n",
    "        logging.info(f\"Genre columns used for user genre features: {genre_columns}\")\n",
    "        \n",
    "        if genre_columns:  # Check if there are genre columns to process\n",
    "            user_genre_means = df.groupby(['userId'])[genre_columns].mean()\n",
    "            user_genre_means.columns = [f'user_mean_{col}' for col in user_genre_means.columns]\n",
    "            \n",
    "            df = pd.merge(df, user_genre_means, on='userId', how='left')\n",
    "            logging.info(\"User-genre interaction features added.\")\n",
    "        else:\n",
    "            logging.warning(\"No genre columns found for user genre features.\")\n",
    "    else:\n",
    "        logging.error(\"UserId column not available for interaction features.\")\n",
    "    \n",
    "    logging.info(f\"Data after adding user genre features: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def get_item_features(df):\n",
    "    # Select only numeric columns for aggregation\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_columns = [col for col in numeric_cols if col not in ['userId', 'movieId', 'rating']]\n",
    "\n",
    "    logging.info(f\"Numeric feature columns used for item features: {feature_columns}\")\n",
    "\n",
    "    item_features = df.groupby('movieId')[feature_columns].mean()\n",
    "\n",
    "    for movie_id, features in item_features.iterrows():\n",
    "        if features.shape[0] != len(feature_columns):\n",
    "            logging.warning(f\"Movie {movie_id} has an unexpected feature vector shape: {features.shape}\")\n",
    "    \n",
    "    return item_features.to_dict('index')\n",
    "\n",
    "def derive_user_preferences(df):\n",
    "    preferences = {}\n",
    "    for user_id, group in df.groupby('userId'):\n",
    "        preferred_movies = group[group['rating'] > 4]['movieId'].tolist()\n",
    "        preferences[user_id] = {'preferred_movies': preferred_movies}\n",
    "    return preferences\n",
    "\n",
    "def generate_recommendations(df, movies_df, item_features, n=5):\n",
    "    recommendations = {}\n",
    "    all_movie_ids = df['movieId'].unique()\n",
    "    \n",
    "    for user in df['userId'].unique():\n",
    "        user_rated_movies = df[df['userId'] == user]['movieId'].unique()\n",
    "        candidate_movies = np.setdiff1d(all_movie_ids, user_rated_movies)\n",
    "        candidate_movies_with_features = [movie for movie in candidate_movies if movie in item_features]\n",
    "        predictions = [(movie, np.random.uniform(3, 5)) for movie in candidate_movies_with_features]\n",
    "        top_recommendations = sorted(predictions, key=lambda x: x[1], reverse=True)[:n]\n",
    "        \n",
    "        if len(top_recommendations) < n:\n",
    "            popular_movies = movies_df['movieId'].value_counts().index.tolist()\n",
    "            fallback_movies = [(movie, np.random.uniform(3, 5)) for movie in popular_movies if movie in item_features and movie not in user_rated_movies and movie not in [rec[0] for rec in top_recommendations]]\n",
    "            top_recommendations.extend(fallback_movies[:n - len(top_recommendations)])\n",
    "        \n",
    "        logging.info(f\"User {user} recommendations: {[rec[0] for rec in top_recommendations]}\")\n",
    "        recommendations[user] = top_recommendations\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    diversity_scores = []\n",
    "\n",
    "    for user, items in recommendations.items():\n",
    "        features = []\n",
    "        missing_features = []\n",
    "        \n",
    "        for item in items:\n",
    "            movie_id = item[0]\n",
    "            if movie_id in item_features:\n",
    "                features.append(list(item_features[movie_id].values()))  # Ensure we are appending numerical values\n",
    "            else:\n",
    "                missing_features.append(movie_id)\n",
    "        \n",
    "        if missing_features:\n",
    "            logging.warning(f\"Missing features for movies {missing_features} for user {user}\")\n",
    "\n",
    "        features_array = np.array(features)\n",
    "        \n",
    "        if features_array.ndim == 1:\n",
    "            features_array = features_array.reshape(1, -1)\n",
    "        \n",
    "        if features_array.shape[0] > 1:\n",
    "            distances = pdist(features_array, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(squareform(distances)))\n",
    "        else:\n",
    "            logging.warning(f\"Not enough features for user {user} with feature array shape: {features_array.shape}. Skipping distance calculation.\")\n",
    "            logging.info(f\"User {user} has feature array: {features_array}\")\n",
    "    \n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features):\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.3f}\")\n",
    "    print(f\"Serendipity: {serendipity:.3f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.3f}\")\n",
    "\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item[0] for item in items])\n",
    "    coverage = len(recommended_items) / catalog_size\n",
    "    return coverage\n",
    "\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            popularity = item_popularity.get(item_id, 0)\n",
    "            if popularity > 0:\n",
    "                novelty_scores.append(-np.log(popularity))\n",
    "            else:\n",
    "                novelty_scores.append(0)  # Handle items with no prior recommendations\n",
    "    return np.mean(novelty_scores) if novelty_scores else 0\n",
    "\n",
    "def calculate_personalization(recommendations):\n",
    "    all_items = set()\n",
    "    for items in recommendations.values():\n",
    "        all_items.update([item[0] for item in items])\n",
    "    \n",
    "    item_list_mapping = {item: idx for idx, item in enumerate(all_items)}\n",
    "    user_item_matrix = []\n",
    "\n",
    "    for items in recommendations.values():\n",
    "        item_vec = [0] * len(all_items)\n",
    "        for item in items:\n",
    "            item_vec[item_list_mapping[item[0]]] = 1\n",
    "        user_item_matrix.append(item_vec)\n",
    "\n",
    "    if user_item_matrix:\n",
    "        jaccard_distances = pdist(user_item_matrix, metric='jaccard')\n",
    "        return 1 - np.mean(jaccard_distances)\n",
    "    return 0\n",
    "\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences):\n",
    "    unexpected_relevant_count = 0\n",
    "    total_relevant_count = 0\n",
    "\n",
    "    for user, items in recommendations.items():\n",
    "        expected_items = expected_recommendations.get(user, [])\n",
    "        preferences = user_preferences.get(user, {})\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_items and item_is_relevant(item_id, preferences):\n",
    "                unexpected_relevant_count += 1\n",
    "            if item_is_relevant(item_id, preferences):\n",
    "                total_relevant_count += 1\n",
    "\n",
    "    return unexpected_relevant_count / total_relevant_count if total_relevant_count else 0\n",
    "\n",
    "def calculate_item_popularity(recommendations):\n",
    "    item_popularity = {}\n",
    "    total_recommendations = 0\n",
    "\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            if item_id in item_popularity:\n",
    "                item_popularity[item_id] += 1\n",
    "            else:\n",
    "                item_popularity[item_id] = 1\n",
    "            total_recommendations += 1\n",
    "\n",
    "    for movie_id in item_popularity:\n",
    "        item_popularity[movie_id] /= total_recommendations\n",
    "\n",
    "    return item_popularity\n",
    "\n",
    "def get_expected_recommendations(df, n_most_popular=100):\n",
    "    most_popular = df['movieId'].value_counts().head(n_most_popular).index.tolist()\n",
    "    return {user: most_popular for user in df['userId'].unique()}\n",
    "\n",
    "def item_is_relevant(item_id, user_preferences):\n",
    "    return item_id in user_preferences.get('preferred_movies', [])\n",
    "\n",
    "def main():\n",
    "    # Configure logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    merged_df, movies_df = load_and_merge_data('../data/movies.csv', '../data/ratings.csv')\n",
    "    if merged_df is not None:\n",
    "        # Process and enhance data\n",
    "        merged_df = preprocess_data(merged_df)\n",
    "        merged_df = add_user_genre_features(merged_df)\n",
    "\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "        recommendations = generate_recommendations(merged_df, movies_df, item_features)\n",
    "        item_popularity = calculate_item_popularity(recommendations)\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "\n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features)\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30344ff5-ca2c-49cf-baa0-0b693f1e596d",
   "metadata": {},
   "source": [
    "# PRIOR MASTER CODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9341aae9-0b7b-4dc0-8bb5-431bd774fcb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import itertools\n",
    "\n",
    "# Constants\n",
    "MOVIES_FILE = '../data/movies.csv'\n",
    "RATINGS_FILE = '../data/ratings.csv'\n",
    "MAX_RATINGS = None\n",
    "N_RECOMMENDATIONS = 5\n",
    "YEAR_BINS = list(range(1900, 2025, 5))\n",
    "RELEASE_YEAR_WEIGHT_DIVISOR = 100\n",
    "RATING_THRESHOLD = 4.0\n",
    "\n",
    "def load_and_merge_data(movies_file, ratings_file, max_ratings=None):\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file, usecols=['movieId', 'title', 'genres'])\n",
    "        ratings_df = pd.read_csv(ratings_file, usecols=['userId', 'movieId', 'rating']).head(max_ratings)\n",
    "\n",
    "        movies_df.drop_duplicates(subset='movieId', inplace=True)\n",
    "        ratings_df.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        merged_df['uniqueId'] = merged_df['userId'].astype(str) + \"_\" + merged_df['movieId'].astype(str)\n",
    "\n",
    "        logging.info(\"Data loaded and merged successfully.\")\n",
    "        return merged_df, movies_df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: {e}. File not found. Check the file paths.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_release_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    if 'title' in df.columns:\n",
    "        df['release_year'] = df['title'].apply(extract_release_year)\n",
    "        df['release_year'].fillna(0, inplace=True)\n",
    "        logging.info(\"Release year feature added.\")\n",
    "    else:\n",
    "        logging.error(\"Title column not available for processing.\")\n",
    "    \n",
    "    if 'genres' in df.columns:\n",
    "        df['genres'] = df['genres'].apply(lambda x: x.split('|'))\n",
    "        genre_dummies = df['genres'].str.join('|').str.get_dummies()\n",
    "        df = pd.concat([df, genre_dummies], axis=1)\n",
    "        logging.info(\"Genre-based features added.\")\n",
    "    else:\n",
    "        logging.error(\"Genres column not available for processing.\")\n",
    "    \n",
    "    df['year_category'] = pd.cut(df['release_year'], bins=YEAR_BINS, labels=YEAR_BINS[:-1], right=False)\n",
    "    year_dummies = pd.get_dummies(df['year_category'], prefix='year')\n",
    "    df = pd.concat([df, year_dummies], axis=1)\n",
    "    df.drop(columns=['year_category'], inplace=True)\n",
    "\n",
    "    logging.info(f\"Data after preprocessing: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def add_user_genre_features(df):\n",
    "    if 'userId' in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        genre_columns = [col for col in numeric_cols if col.startswith('genre_')]\n",
    "        \n",
    "        logging.info(f\"Genre columns used for user genre features: {genre_columns}\")\n",
    "        \n",
    "        if genre_columns:\n",
    "            user_genre_means = df.groupby(['userId'])[genre_columns].mean()\n",
    "            user_genre_means.columns = [f'user_mean_{col}' for col in user_genre_means.columns]\n",
    "            df = pd.merge(df, user_genre_means, on='userId', how='left')\n",
    "            logging.info(\"User-genre interaction features added.\")\n",
    "        else:\n",
    "            logging.warning(\"No genre columns found for user genre features.\")\n",
    "    else:\n",
    "        logging.error(\"UserId column not available for interaction features.\")\n",
    "    \n",
    "    logging.info(f\"Data after adding user genre features: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def get_item_features(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_columns = [col for col in numeric_cols if col not in ['userId', 'movieId', 'rating']]\n",
    "    logging.info(f\"Numeric feature columns used for item features: {feature_columns}\")\n",
    "    item_features = df.groupby('movieId')[feature_columns].mean()\n",
    "    return item_features.to_dict('index')\n",
    "\n",
    "def derive_user_preferences(df):\n",
    "    preferences = {}\n",
    "    for user_id, group in df.groupby('userId'):\n",
    "        preferred_movies = group[group['rating'] > RATING_THRESHOLD]['movieId'].tolist()\n",
    "        preferences[user_id] = {'preferred_movies': preferred_movies}\n",
    "    return preferences\n",
    "\n",
    "def generate_recommendations(algo, df, user_id, n=5):\n",
    "    user_data = df[df['userId'] == user_id]\n",
    "    user_movies = user_data['movieId'].unique()\n",
    "    all_movies = df['movieId'].unique()\n",
    "    possible_movies = np.setdiff1d(all_movies, user_movies)\n",
    "    \n",
    "    recommendations = []\n",
    "    for movie_id in possible_movies:\n",
    "        prediction = algo.predict(user_id, movie_id).est\n",
    "        recommendations.append((movie_id, prediction))\n",
    "    \n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:n]\n",
    "    return recommendations\n",
    "\n",
    "def calculate_item_popularity(recommendations):\n",
    "    item_popularity = {}\n",
    "    total_recommendations = 0\n",
    "    \n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            if item_id in item_popularity:\n",
    "                item_popularity[item_id] += 1\n",
    "            else:\n",
    "                item_popularity[item_id] = 1\n",
    "            total_recommendations += 1\n",
    "    \n",
    "    for movie_id in item_popularity:\n",
    "        item_popularity[movie_id] /= total_recommendations\n",
    "    \n",
    "    return item_popularity\n",
    "\n",
    "def get_expected_recommendations(df, n_most_popular=100):\n",
    "    most_popular = df['movieId'].value_counts().head(n_most_popular).index.tolist()\n",
    "    return {user: most_popular for user in df['userId'].unique()}\n",
    "\n",
    "def calculate_model_metrics(predictions):\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    estimated_ratings = [pred.est for pred in predictions]\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "    y_pred = [1 if est >= RATING_THRESHOLD else 0 for est in estimated_ratings]\n",
    "    y_true = [1 if true_r >= RATING_THRESHOLD else 0 for true_r in true_ratings]\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features, movies_df):\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.3f}\")\n",
    "    print(f\"Serendipity: {serendipity:.3f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.3f}\")\n",
    "\n",
    "    for user, items in list(recommendations.items())[:10]:  # Limit to 10 users\n",
    "        print(f\"User {user} recommendations:\")\n",
    "        for movie_id, pred_rating in items:\n",
    "            title = movies_df[movies_df['movieId'] == movie_id]['title'].values[0]\n",
    "            print(f\"\\tMovie: {title}, Predicted Rating: {pred_rating:.2f}\")\n",
    "\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item[0] for item in items])\n",
    "    coverage = len(recommended_items) / catalog_size\n",
    "    return coverage\n",
    "\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            popularity = item_popularity.get(item_id, 0)\n",
    "            if popularity > 0:\n",
    "                novelty_scores.append(-np.log2(popularity))\n",
    "    return np.mean(novelty_scores) if novelty_scores else 0\n",
    "\n",
    "def calculate_personalization(recommendations):\n",
    "    user_pairs = list(itertools.combinations(recommendations.keys(), 2))\n",
    "    similarity_sum = 0\n",
    "    for user1, user2 in user_pairs:\n",
    "        items1 = {item[0] for item in recommendations[user1]}\n",
    "        items2 = {item[0] for item in recommendations[user2]}\n",
    "        similarity_sum += len(items1.intersection(items2)) / len(items1.union(items2))\n",
    "    personalization = 1 - (similarity_sum / len(user_pairs)) if user_pairs else 1\n",
    "    return personalization\n",
    "\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences):\n",
    "    serendipity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        expected_set = set(expected_recommendations[user])\n",
    "        user_set = set(user_preferences[user]['preferred_movies'])\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_set and item_id not in user_set:\n",
    "                serendipity_scores.append(1)\n",
    "            else:\n",
    "                serendipity_scores.append(0)\n",
    "    return np.mean(serendipity_scores) if serendipity_scores else 0\n",
    "\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    diversity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        features = np.array([list(item_features[item[0]].values()) for item in items if item[0] in item_features])\n",
    "        if features.ndim == 2 and features.shape[1] > 1:\n",
    "            distances = pdist(features, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(squareform(distances)))\n",
    "        else:\n",
    "            logging.error(f\"Error in calculating distances for user {user}: A 2-dimensional array must be passed.\")\n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    merged_df, movies_df = load_and_merge_data(MOVIES_FILE, RATINGS_FILE, MAX_RATINGS)\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = preprocess_data(merged_df)\n",
    "        merged_df = add_user_genre_features(merged_df)\n",
    "\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "\n",
    "        # Load data into Surprise for model training\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(merged_df[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "        # Perform GridSearchCV to find the best parameters\n",
    "        param_grid = {\n",
    "            'n_epochs': [20, 40, 60],\n",
    "            'lr_all': [0.002, 0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.05, 0.1]\n",
    "        }\n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "        gs.fit(data)\n",
    "\n",
    "        best_params = gs.best_params['rmse']\n",
    "        print(f\"Best params: {best_params}\")\n",
    "\n",
    "        algo = SVD(**best_params)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Calculate model metrics\n",
    "        metrics = calculate_model_metrics(predictions)\n",
    "        print(\"Model Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        recommendations = {user_id: generate_recommendations(algo, merged_df, user_id, N_RECOMMENDATIONS) for user_id in merged_df['userId'].unique()}\n",
    "        item_popularity = calculate_item_popularity(recommendations)\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "        \n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features, movies_df)\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94da8a5b-c41a-41c5-bd09-7c9a7b79611a",
   "metadata": {},
   "source": [
    "# Surprise Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc9234-7dd5-4972-9588-a55accc2a4e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import itertools\n",
    "\n",
    "# Constants\n",
    "MOVIES_FILE = '../data/movies.csv'\n",
    "RATINGS_FILE = '../data/ratings.csv'\n",
    "MAX_RATINGS = None\n",
    "N_RECOMMENDATIONS = 5\n",
    "YEAR_BINS = list(range(1900, 2025, 5))\n",
    "RELEASE_YEAR_WEIGHT_DIVISOR = 50\n",
    "RATING_THRESHOLD = 4.0\n",
    "\n",
    "def load_and_merge_data(movies_file, ratings_file, max_ratings=None):\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file, usecols=['movieId', 'title', 'genres'])\n",
    "        ratings_df = pd.read_csv(ratings_file, usecols=['userId', 'movieId', 'rating']).head(max_ratings)\n",
    "\n",
    "        movies_df.drop_duplicates(subset='movieId', inplace=True)\n",
    "        ratings_df.drop_duplicates(subset=['userId', 'movieId'], inplace=True)\n",
    "\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        merged_df['uniqueId'] = merged_df['userId'].astype(str) + \"_\" + merged_df['movieId'].astype(str)\n",
    "\n",
    "        logging.info(\"Data loaded and merged successfully.\")\n",
    "        return merged_df, movies_df\n",
    "    except FileNotFoundError as e:\n",
    "        logging.error(f\"Error: {e}. File not found. Check the file paths.\")\n",
    "        return None, None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_release_year(title):\n",
    "    match = re.search(r'\\((\\d{4})\\)', title)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def preprocess_data(df):\n",
    "    if 'title' in df.columns:\n",
    "        df['release_year'] = df['title'].apply(extract_release_year)\n",
    "        df['release_year'].fillna(0, inplace=True)\n",
    "        logging.info(\"Release year feature added.\")\n",
    "    else:\n",
    "        logging.error(\"Title column not available for processing.\")\n",
    "    \n",
    "    if 'genres' in df.columns:\n",
    "        df['genres'] = df['genres'].apply(lambda x: x.split('|'))\n",
    "        genre_dummies = df['genres'].str.join('|').str.get_dummies()\n",
    "        df = pd.concat([df, genre_dummies], axis=1)\n",
    "        logging.info(\"Genre-based features added.\")\n",
    "    else:\n",
    "        logging.error(\"Genres column not available for processing.\")\n",
    "    \n",
    "    df['year_category'] = pd.cut(df['release_year'], bins=YEAR_BINS, labels=YEAR_BINS[:-1], right=False)\n",
    "    year_dummies = pd.get_dummies(df['year_category'], prefix='year')\n",
    "    df = pd.concat([df, year_dummies], axis=1)\n",
    "    df.drop(columns=['year_category'], inplace=True)\n",
    "\n",
    "    logging.info(f\"Data after preprocessing: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def add_user_genre_features(df):\n",
    "    if 'userId' in df.columns:\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        genre_columns = [col for col in numeric_cols if col.startswith('genre_')]\n",
    "        \n",
    "        logging.info(f\"Genre columns used for user genre features: {genre_columns}\")\n",
    "        \n",
    "        if genre_columns:\n",
    "            user_genre_means = df.groupby(['userId'])[genre_columns].mean()\n",
    "            user_genre_means.columns = [f'user_mean_{col}' for col in user_genre_means.columns]\n",
    "            df = pd.merge(df, user_genre_means, on='userId', how='left')\n",
    "            logging.info(\"User-genre interaction features added.\")\n",
    "        else:\n",
    "            logging.warning(\"No genre columns found for user genre features.\")\n",
    "    else:\n",
    "        logging.error(\"UserId column not available for interaction features.\")\n",
    "    \n",
    "    logging.info(f\"Data after adding user genre features: {df.head()}\")\n",
    "    return df\n",
    "\n",
    "def get_item_features(df):\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    feature_columns = [col for col in numeric_cols if col not in ['userId', 'movieId', 'rating']]\n",
    "    logging.info(f\"Numeric feature columns used for item features: {feature_columns}\")\n",
    "    item_features = df.groupby('movieId')[feature_columns].mean()\n",
    "    return item_features.to_dict('index')\n",
    "\n",
    "def derive_user_preferences(df):\n",
    "    preferences = {}\n",
    "    for user_id, group in df.groupby('userId'):\n",
    "        preferred_movies = group[group['rating'] > RATING_THRESHOLD]['movieId'].tolist()\n",
    "        preferences[user_id] = {'preferred_movies': preferred_movies}\n",
    "    return preferences\n",
    "\n",
    "def generate_recommendations(algo, df, user_id, n=5):\n",
    "    user_data = df[df['userId'] == user_id]\n",
    "    user_movies = user_data['movieId'].unique()\n",
    "    all_movies = df['movieId'].unique()\n",
    "    possible_movies = np.setdiff1d(all_movies, user_movies)\n",
    "    \n",
    "    recommendations = []\n",
    "    for movie_id in possible_movies:\n",
    "        prediction = algo.predict(user_id, movie_id).est\n",
    "        recommendations.append((movie_id, prediction))\n",
    "    \n",
    "    recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)[:n]\n",
    "    return recommendations\n",
    "\n",
    "def calculate_item_popularity(recommendations):\n",
    "    item_popularity = {}\n",
    "    total_recommendations = 0\n",
    "    \n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            if item_id in item_popularity:\n",
    "                item_popularity[item_id] += 1\n",
    "            else:\n",
    "                item_popularity[item_id] = 1\n",
    "            total_recommendations += 1\n",
    "    \n",
    "    for movie_id in item_popularity:\n",
    "        item_popularity[movie_id] /= total_recommendations\n",
    "    \n",
    "    return item_popularity\n",
    "\n",
    "def get_expected_recommendations(df, n_most_popular=100):\n",
    "    most_popular = df['movieId'].value_counts().head(n_most_popular).index.tolist()\n",
    "    return {user: most_popular for user in df['userId'].unique()}\n",
    "\n",
    "def calculate_model_metrics(predictions):\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    estimated_ratings = [pred.est for pred in predictions]\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "    y_pred = [1 if est >= RATING_THRESHOLD else 0 for est in estimated_ratings]\n",
    "    y_true = [1 if true_r >= RATING_THRESHOLD else 0 for true_r in true_ratings]\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features, movies_df):\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.3f}\")\n",
    "    print(f\"Serendipity: {serendipity:.3f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.3f}\")\n",
    "\n",
    "    for user, items in list(recommendations.items())[:10]:  # Limit to 10 users\n",
    "        print(f\"User {user} recommendations:\")\n",
    "        for movie_id, pred_rating in items:\n",
    "            title = movies_df[movies_df['movieId'] == movie_id]['title'].values[0]\n",
    "            print(f\"\\tMovie: {title}, Predicted Rating: {pred_rating:.2f}\")\n",
    "\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item[0] for item in items])\n",
    "    coverage = len(recommended_items) / catalog_size\n",
    "    return coverage\n",
    "\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            popularity = item_popularity.get(item_id, 0)\n",
    "            if popularity > 0:\n",
    "                novelty_scores.append(-np.log2(popularity))\n",
    "    return np.mean(novelty_scores) if novelty_scores else 0\n",
    "\n",
    "def calculate_personalization(recommendations):\n",
    "    user_pairs = list(itertools.combinations(recommendations.keys(), 2))\n",
    "    similarity_sum = 0\n",
    "    for user1, user2 in user_pairs:\n",
    "        items1 = {item[0] for item in recommendations[user1]}\n",
    "        items2 = {item[0] for item in recommendations[user2]}\n",
    "        similarity_sum += len(items1.intersection(items2)) / len(items1.union(items2))\n",
    "    personalization = 1 - (similarity_sum / len(user_pairs)) if user_pairs else 1\n",
    "    return personalization\n",
    "\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences):\n",
    "    serendipity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        expected_set = set(expected_recommendations[user])\n",
    "        user_set = set(user_preferences[user]['preferred_movies'])\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_set and item_id not in user_set:\n",
    "                serendipity_scores.append(1)\n",
    "            else:\n",
    "                serendipity_scores.append(0)\n",
    "    return np.mean(serendipity_scores) if serendipity_scores else 0\n",
    "\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    diversity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        features = np.array([list(item_features[item[0]].values()) for item in items if item[0] in item_features])\n",
    "        if features.ndim == 2 and features.shape[1] > 1:\n",
    "            distances = pdist(features, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(squareform(distances)))\n",
    "        else:\n",
    "            logging.error(f\"Error in calculating distances for user {user}: A 2-dimensional array must be passed.\")\n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "def main():\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    merged_df, movies_df = load_and_merge_data(MOVIES_FILE, RATINGS_FILE, MAX_RATINGS)\n",
    "    \n",
    "    if merged_df is not None:\n",
    "        merged_df = preprocess_data(merged_df)\n",
    "        merged_df = add_user_genre_features(merged_df)\n",
    "\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "\n",
    "        # Load data into Surprise for model training\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(merged_df[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "        # Perform GridSearchCV to find the best parameters\n",
    "        param_grid = {\n",
    "            'n_epochs': [20, 40, 60],\n",
    "            'lr_all': [0.002, 0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.05, 0.1]\n",
    "        }\n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n",
    "        gs.fit(data)\n",
    "\n",
    "        best_params = gs.best_params['rmse']\n",
    "        print(f\"Best params: {best_params}\")\n",
    "\n",
    "        algo = SVD(**best_params)\n",
    "        algo.fit(trainset)\n",
    "        predictions = algo.test(testset)\n",
    "\n",
    "        # Calculate model metrics\n",
    "        metrics = calculate_model_metrics(predictions)\n",
    "        print(\"Model Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        recommendations = {user_id: generate_recommendations(algo, merged_df, user_id, N_RECOMMENDATIONS) for user_id in merged_df['userId'].unique()}\n",
    "        item_popularity = calculate_item_popularity(recommendations)\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "        \n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features, movies_df)\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a357b03-04df-4e20-9980-76a5f7f4685a",
   "metadata": {},
   "source": [
    "# BEST CODE!  WITH MODERATE DOCUMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c8ad2-8ad5-4662-bbc9-ab9f180b5ed3",
   "metadata": {},
   "source": [
    "Detailed Comments and Documentation:\n",
    "Import Libraries: Necessary libraries for data processing, model training, and evaluation are imported.\n",
    "\n",
    "Constants: Constants for file paths, model parameters, and other settings are defined.\n",
    "\n",
    "load_and_merge_data: Function to load and merge movie and rating data from CSV files.\n",
    "\n",
    "extract_release_year: Function to extract the release year from the movie title.\n",
    "\n",
    "preprocess_data: Function to preprocess the data by adding release year and genre features.\n",
    "\n",
    "add_user_genre_features: Function to add user-specific genre interaction features.\n",
    "\n",
    "get_item_features: Function to extract item features from the dataset.\n",
    "\n",
    "derive_user_preferences: Function to derive user preferences based on their ratings.\n",
    "\n",
    "generate_recommendations: Function to generate movie recommendations for a given user using a trained algorithm.\n",
    "\n",
    "calculate_item_popularity: Function to calculate item popularity based on recommendations.\n",
    "\n",
    "get_expected_recommendations: Function to get expected recommendations based on item popularity.\n",
    "\n",
    "calculate_model_metrics: Function to calculate various model metrics (RMSE, MAE, Precision, Recall, F1 Score).\n",
    "\n",
    "display_metrics: Function to display various metrics for the recommendations.\n",
    "\n",
    "calculate_coverage: Function to calculate coverage of recommendations.\n",
    "\n",
    "calculate_novelty: Function to calculate novelty of recommendations.\n",
    "\n",
    "calculate_personalization: Function to calculate personalization of recommendations.\n",
    "\n",
    "calculate_serendipity: Function to calculate serendipity of recommendations.\n",
    "\n",
    "calculate_intra_list_similarity: Function to calculate intra-list similarity of recommendations.\n",
    "\n",
    "main: Main function to execute the recommendation system. This function loads and preprocesses data, trains the model, makes predictions, and displays recommendations and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b821e099-0d66-4d08-a0ca-8b0603da67bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca4af9-727f-41dd-95f5-99c0920fac84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Constants\n",
    "MOVIES_FILE = '../data/movies.csv'\n",
    "RATINGS_FILE = '../data/ratings.csv'\n",
    "N_RECOMMENDATIONS = 5  # Changed from 10 to 5\n",
    "YEAR_DIVISOR = 0.05  # Changed from 5 to 0.05\n",
    "RATING_THRESHOLD = 4.0  # Changed from 3.5 to 4.0\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "def load_data(movies_file, ratings_file):\n",
    "    \"\"\"Load movies and ratings datasets and merge them.\"\"\"\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file)\n",
    "        ratings_df = pd.read_csv(ratings_file)\n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        # Ensure release_year column is extracted from title column if not present\n",
    "        if 'release_year' not in merged_df.columns:\n",
    "            merged_df['release_year'] = merged_df['title'].str.extract(r'\\((\\d{4})\\)').astype(float)\n",
    "        return merged_df, movies_df, ratings_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Calculate weighted release year\n",
    "def get_weighted_release_year(year, divisor):\n",
    "    \"\"\"Calculate the weighted release year by dividing the year by the given divisor.\"\"\"\n",
    "    return year * divisor\n",
    "\n",
    "# Get item features\n",
    "def get_item_features(df):\n",
    "    \"\"\"Extract item features from the DataFrame.\"\"\"\n",
    "    if 'release_year' not in df.columns:\n",
    "        raise KeyError(\"release_year column is missing from the DataFrame.\")\n",
    "    df['release_year_bucket'] = df['release_year'].apply(lambda x: get_weighted_release_year(x, YEAR_DIVISOR))\n",
    "    genre_columns = [col for col in df.columns if col.startswith('user_mean_')]\n",
    "    item_features = df[['movieId', 'release_year_bucket'] + genre_columns].drop_duplicates().set_index('movieId')\n",
    "    item_features_dict = item_features.to_dict(orient='index')\n",
    "    return item_features_dict\n",
    "\n",
    "# Derive user preferences\n",
    "def derive_user_preferences(df):\n",
    "    \"\"\"Calculate mean ratings for each genre and release year for each user.\"\"\"\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    user_genre_means = df.groupby('userId')[numeric_columns].mean().add_prefix('user_mean_')\n",
    "    return user_genre_means.to_dict(orient='index')\n",
    "\n",
    "# Generate recommendations\n",
    "def generate_recommendations(algo, df, user_id, n_recommendations):\n",
    "    \"\"\"Generate top N recommendations for a given user.\"\"\"\n",
    "    user_rated_items = df[df['userId'] == user_id]['movieId'].tolist()\n",
    "    all_items = df['movieId'].unique()\n",
    "    recommendations = []\n",
    "    for item_id in all_items:\n",
    "        if item_id not in user_rated_items:\n",
    "            pred = algo.predict(user_id, item_id)\n",
    "            recommendations.append((item_id, pred.est))\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n_recommendations]\n",
    "\n",
    "# Calculate item popularity\n",
    "def calculate_item_popularity(recommendations):\n",
    "    \"\"\"Calculate how often each item is recommended.\"\"\"\n",
    "    item_popularity = {}\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            item_popularity[item_id] = item_popularity.get(item_id, 0) + 1\n",
    "    return item_popularity\n",
    "\n",
    "# Expected recommendations for novelty\n",
    "def get_expected_recommendations(df):\n",
    "    \"\"\"Get expected recommendations based on high ratings.\"\"\"\n",
    "    return df[df['rating'] >= RATING_THRESHOLD]['movieId'].unique()\n",
    "\n",
    "# Calculate model metrics\n",
    "def calculate_model_metrics(predictions):\n",
    "    \"\"\"Calculate various metrics to evaluate the model.\"\"\"\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    estimated_ratings = [pred.est for pred in predictions]\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "    y_true = [1 if true_r >= RATING_THRESHOLD else 0 for true_r in true_ratings]\n",
    "    y_pred = [1 if est >= RATING_THRESHOLD else 0 for est in estimated_ratings]\n",
    "    precision = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_pred)\n",
    "    recall = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_true)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Calculate coverage\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    \"\"\"Calculate the percentage of items in the catalog that have been recommended.\"\"\"\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item_id for item_id, _ in items])\n",
    "    return len(recommended_items) / catalog_size\n",
    "\n",
    "# Calculate novelty\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    \"\"\"Calculate the average popularity of recommended items.\"\"\"\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            novelty_scores.append(item_popularity.get(item_id, 0))\n",
    "    return np.mean(novelty_scores)\n",
    "\n",
    "# Calculate personalization\n",
    "def calculate_personalization(recommendations):\n",
    "    \"\"\"Calculate how different the recommendations are for different users.\"\"\"\n",
    "    user_pairs = list(itertools.combinations(recommendations.keys(), 2))\n",
    "    similarity_sum = 0\n",
    "    for user1, user2 in user_pairs:\n",
    "        items1 = {item_id for item_id, _ in recommendations[user1]}\n",
    "        items2 = {item_id for item_id, _ in recommendations[user2]}\n",
    "        similarity_sum += len(items1 & items2) / len(items1 | items2)\n",
    "    return 1 - (similarity_sum / len(user_pairs))\n",
    "\n",
    "# Calculate serendipity\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features):\n",
    "    \"\"\"Calculate the serendipity of the recommendations.\"\"\"\n",
    "    serendipity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        user_prefs = user_preferences.get(user, {})\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_recommendations:\n",
    "                item_genres = item_features.get(item_id, {})\n",
    "                similarity = sum(user_prefs.get(f\"user_mean_{genre}\", 0) * item_genres.get(genre, 0) for genre in item_genres)\n",
    "                serendipity_scores.append(1 - similarity)\n",
    "    return np.mean(serendipity_scores) if serendipity_scores else 0\n",
    "\n",
    "# Calculate intra-list diversity\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    \"\"\"Calculate the diversity within a single user's list of recommendations.\"\"\"\n",
    "    diversity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        features = [list(item_features[item[0]].values()) for item in items if item[0] in item_features]\n",
    "        if len(features) > 1:\n",
    "            distances = pdist(features, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(squareform(distances)))\n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "# Display metrics\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features):\n",
    "    \"\"\"Display various metrics to evaluate the recommendations.\"\"\"\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.4f}\")\n",
    "    print(f\"Serendipity: {serendipity:.4f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.4f}\")\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    \"\"\"Main function to load data, train the model, and generate recommendations.\"\"\"\n",
    "    merged_df, movies_df, ratings_df = load_data(MOVIES_FILE, RATINGS_FILE)\n",
    "    if merged_df is not None:\n",
    "        # Load data into Surprise for model training\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(merged_df[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.2)\n",
    "\n",
    "        # Hyperparameter tuning with GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_factors': [50, 100],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.1]\n",
    "        }\n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3)\n",
    "        gs.fit(data)\n",
    "        best_params = gs.best_params['rmse']\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        # Train the best model\n",
    "        algo = SVD(**best_params)\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions = algo.test(testset)\n",
    "        metrics = calculate_model_metrics(predictions)\n",
    "        print(\"Model Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Generate recommendations for all users\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "        recommendations = {user_id: generate_recommendations(algo, merged_df, user_id, N_RECOMMENDATIONS) for user_id in merged_df['userId'].unique()}\n",
    "        item_popularity = calculate_item_popularity(recommendations)\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "\n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features)\n",
    "\n",
    "        # Print recommendations for the first 10 users\n",
    "        for user_id, recs in list(recommendations.items())[:10]:\n",
    "            print(f\"User {user_id} recommendations:\")\n",
    "            for movie_id, est_rating in recs:\n",
    "                title = movies_df[movies_df['movieId'] == movie_id]['title'].values[0]\n",
    "                print(f\"  {title}: {est_rating:.2f}\")\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bc48b-bc88-4b9f-865a-cf5e82dba186",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3969f445-c0a2-4e28-a1a6-be7e2d6c64ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. You can now perform EDA on the loaded datasets.\n",
      "\n",
      "Movies DataFrame:\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n",
      "\n",
      "Ratings DataFrame:\n",
      "     userId  movieId  rating   timestamp\n",
      "261       3       31     0.5  1306463578\n",
      "262       3      527     0.5  1306464275\n",
      "263       3      647     0.5  1306463619\n",
      "264       3      688     0.5  1306464228\n",
      "265       3      720     0.5  1306463595\n",
      "\n",
      "Merged DataFrame:\n",
      "   userId  movieId  rating   timestamp                   title genres  \\\n",
      "0       3       31     0.5  1306463578  Dangerous Minds (1995)  Drama   \n",
      "1      40       31     4.0   832059371  Dangerous Minds (1995)  Drama   \n",
      "2     362       31     4.0  1530642798  Dangerous Minds (1995)  Drama   \n",
      "3     483       31     2.0  1181495275  Dangerous Minds (1995)  Drama   \n",
      "4     599       31     2.0  1498511120  Dangerous Minds (1995)  Drama   \n",
      "\n",
      "   release_year  Documentary  Action  (no genres listed)  ...  Crime  Western  \\\n",
      "0        1995.0            0       0                   0  ...      0        0   \n",
      "1        1995.0            0       0                   0  ...      0        0   \n",
      "2        1995.0            0       0                   0  ...      0        0   \n",
      "3        1995.0            0       0                   0  ...      0        0   \n",
      "4        1995.0            0       0                   0  ...      0        0   \n",
      "\n",
      "   Musical  Thriller  Sci-Fi  Children  War  Comedy  Animation  Horror  \n",
      "0        0         0       0         0    0       0          0       0  \n",
      "1        0         0       0         0    0       0          0       0  \n",
      "2        0         0       0         0    0       0          0       0  \n",
      "3        0         0       0         0    0       0          0       0  \n",
      "4        0         0       0         0    0       0          0       0  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "\n",
      "Ratings Distribution:\n",
      "rating\n",
      "0.5     320\n",
      "1.0     583\n",
      "1.5     685\n",
      "2.0    1803\n",
      "2.5    2023\n",
      "3.0    4554\n",
      "3.5    3697\n",
      "4.0    6012\n",
      "4.5    1982\n",
      "5.0    2438\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Number of Ratings per User:\n",
      "count     100.000000\n",
      "mean      240.970000\n",
      "std       367.934872\n",
      "min        20.000000\n",
      "25%        38.250000\n",
      "50%       108.000000\n",
      "75%       281.500000\n",
      "max      2478.000000\n",
      "Name: count, dtype: float64\n",
      "Best parameters: {'n_factors': 50, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.1, 'biased': True}\n",
      "Model Metrics:\n",
      "RMSE: 0.8572\n",
      "MAE: 0.6596\n",
      "Precision: 0.8152\n",
      "Recall: 0.2572\n",
      "F1 Score: 0.3910\n",
      "Item features:\n",
      "         release_year_bucket   timestamp  release_year  Documentary  Action  \\\n",
      "movieId                                                                       \n",
      "31                  0.766129  1306463578        1995.0            0       0   \n",
      "527                 0.750000  1306464275        1993.0            0       0   \n",
      "647                 0.774194  1306463619        1996.0            0       1   \n",
      "688                 0.766129  1306464228        1995.0            0       1   \n",
      "720                 0.774194  1306463595        1996.0            0       0   \n",
      "\n",
      "         (no genres listed)  Film-Noir  Fantasy  IMAX  Mystery  ...  Crime  \\\n",
      "movieId                                                         ...          \n",
      "31                        0          0        0     0        0  ...      0   \n",
      "527                       0          0        0     0        0  ...      0   \n",
      "647                       0          0        0     0        0  ...      1   \n",
      "688                       0          0        0     0        0  ...      0   \n",
      "720                       0          0        0     0        0  ...      0   \n",
      "\n",
      "         Western  Musical  Thriller  Sci-Fi  Children  War  Comedy  Animation  \\\n",
      "movieId                                                                         \n",
      "31             0        0         0       0         0    0       0          0   \n",
      "527            0        0         0       0         0    1       0          0   \n",
      "647            0        0         0       0         0    1       0          0   \n",
      "688            0        0         0       0         0    1       1          0   \n",
      "720            0        0         0       0         0    0       1          1   \n",
      "\n",
      "         Horror  \n",
      "movieId          \n",
      "31            0  \n",
      "527           0  \n",
      "647           0  \n",
      "688           0  \n",
      "720           0  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Catalog Coverage: 0.41%\n",
      "Average Novelty: 21.4620\n",
      "Personalization: 0.7501\n",
      "Serendipity: 0.0000\n",
      "Intra-list Diversity: 1.0000\n",
      "Hit Rate: 0.0000\n",
      "Mean Reciprocal Rank (MRR): 0.0000\n",
      "Average Precision (AP): 0.0000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 379\u001b[0m\n\u001b[0;32m    376\u001b[0m display_metrics(recommendations, \u001b[38;5;28mlen\u001b[39m(movies_df), item_popularity, expected_recommendations, user_preferences, item_features, merged_df, N_RECOMMENDATIONS)\n\u001b[0;32m    378\u001b[0m \u001b[38;5;66;03m# Calculate and display recommendation metrics\u001b[39;00m\n\u001b[1;32m--> 379\u001b[0m recommendation_metrics \u001b[38;5;241m=\u001b[39m calculate_recommendation_metrics(recommendations, merged_df)\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecommendation Metrics:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric, value \u001b[38;5;129;01min\u001b[39;00m recommendation_metrics\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[17], line 291\u001b[0m, in \u001b[0;36mcalculate_recommendation_metrics\u001b[1;34m(recommendations, df)\u001b[0m\n\u001b[0;32m    288\u001b[0m             y_true\u001b[38;5;241m.\u001b[39mappend(actual_rating[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    289\u001b[0m             y_pred\u001b[38;5;241m.\u001b[39mappend(predicted_rating)\n\u001b[1;32m--> 291\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(y_true, y_pred))\n\u001b[0;32m    292\u001b[0m mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_true, y_pred)\n\u001b[0;32m    293\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score([\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m RATING_THRESHOLD \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_true], [\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m RATING_THRESHOLD \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m y_pred], zero_division\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mC:\\users\\trobb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    207\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    208\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    209\u001b[0m         )\n\u001b[0;32m    210\u001b[0m     ):\n\u001b[1;32m--> 211\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    221\u001b[0m     )\n",
      "File \u001b[1;32mC:\\users\\trobb\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:474\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    405\u001b[0m     {\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    415\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    416\u001b[0m ):\n\u001b[0;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    418\u001b[0m \n\u001b[0;32m    419\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 474\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m _check_reg_targets(\n\u001b[0;32m    475\u001b[0m         y_true, y_pred, multioutput\n\u001b[0;32m    476\u001b[0m     )\n\u001b[0;32m    477\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    478\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mC:\\users\\trobb\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;124;03m    correct keyword.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m--> 100\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    101\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mC:\\users\\trobb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:969\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    967\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m--> 969\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    970\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    971\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    972\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m    973\u001b[0m         )\n\u001b[0;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m    976\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0,)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "import re\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Constants\n",
    "MOVIES_FILE = '../data/movies.csv'\n",
    "RATINGS_FILE = '../data/ratings.csv'\n",
    "N_RECOMMENDATIONS = 5\n",
    "YEAR_DIVISOR = 1.0  # Adjust this value based on experimentation\n",
    "RATING_THRESHOLD = 4.0\n",
    "RANDOM_SEED = 42  # Random seed for reproducibility\n",
    "USER_SAMPLE_SIZE = 100  # Set sample size to 100 for now\n",
    "\n",
    "# Weights for hybrid scoring\n",
    "CF_WEIGHT = 0.7\n",
    "CBF_WEIGHT = 0.3\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "def load_data(movies_file, ratings_file, user_sample_size=None):\n",
    "    \"\"\"Load movies and ratings datasets and merge them. Optionally sample a subset of users.\"\"\"\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file)\n",
    "        ratings_df = pd.read_csv(ratings_file)\n",
    "        \n",
    "        if user_sample_size:\n",
    "            unique_users = ratings_df['userId'].drop_duplicates()\n",
    "            actual_sample_size = min(user_sample_size, len(unique_users))\n",
    "            sampled_users = unique_users.sample(n=actual_sample_size, random_state=RANDOM_SEED)\n",
    "            ratings_df = ratings_df[ratings_df['userId'].isin(sampled_users)]\n",
    "        \n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        if 'release_year' not in merged_df.columns:\n",
    "            merged_df['release_year'] = merged_df['title'].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "        \n",
    "        # Ensure proper genre handling\n",
    "        genre_list = list(set(itertools.chain.from_iterable(merged_df['genres'].str.split('|'))))\n",
    "        for genre in genre_list:\n",
    "            genre_pattern = re.escape(genre)  # Escape genre to treat it as a literal string\n",
    "            merged_df[genre] = merged_df['genres'].str.contains(r'\\b' + genre_pattern + r'\\b').astype(int)\n",
    "        return merged_df, movies_df, ratings_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Calculate weighted release year\n",
    "def get_weighted_release_year(year, divisor):\n",
    "    \"\"\"Calculate the weighted release year by normalizing and then dividing the year by the given divisor.\"\"\"\n",
    "    min_year = 1900  # Assuming movies are not older than 1900\n",
    "    max_year = 2024  # Use the current year as the upper bound\n",
    "    normalized_year = (year - min_year) / (max_year - min_year)\n",
    "    return normalized_year / divisor\n",
    "\n",
    "# Get item features\n",
    "def get_item_features(df):\n",
    "    \"\"\"Extract item features from the DataFrame.\"\"\"\n",
    "    if 'release_year' not in df.columns:\n",
    "        raise KeyError(\"release_year column is missing from the DataFrame.\")\n",
    "    df['release_year_bucket'] = df['release_year'].apply(lambda x: get_weighted_release_year(x, YEAR_DIVISOR))\n",
    "    genre_columns = [col for col in df.columns if col not in ['userId', 'movieId', 'rating', 'title', 'genres', 'release_year_bucket']]\n",
    "    item_features = df[['movieId', 'release_year_bucket'] + genre_columns].drop_duplicates().set_index('movieId')\n",
    "    # Ensure index is unique\n",
    "    item_features = item_features.loc[~item_features.index.duplicated(keep='first')]\n",
    "    # Handle missing values by filling them with zeros or an appropriate value\n",
    "    item_features = item_features.fillna(0)\n",
    "    # Debugging: Print item features to ensure correctness\n",
    "    print(\"Item features:\")\n",
    "    print(item_features.head())\n",
    "    item_features_dict = item_features.to_dict(orient='index')\n",
    "    return item_features_dict\n",
    "\n",
    "# Derive user preferences\n",
    "def derive_user_preferences(df):\n",
    "    \"\"\"Calculate mean ratings for each genre and release year for each user.\"\"\"\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    user_genre_means = df.groupby('userId')[numeric_columns].mean().add_prefix('user_mean_')\n",
    "    return user_genre_means.to_dict(orient='index')\n",
    "\n",
    "# Normalize content scores\n",
    "def normalize_scores(scores):\n",
    "    \"\"\"Normalize scores to be between 0.5 and 5.0.\"\"\"\n",
    "    scores = np.array(scores)\n",
    "    min_score = np.nanmin(scores)\n",
    "    max_score = np.nanmax(scores)\n",
    "    if max_score != min_score:\n",
    "        normalized_scores = 0.5 + 4.5 * ((scores - min_score) / (max_score - min_score))\n",
    "    else:\n",
    "        normalized_scores = scores\n",
    "    return normalized_scores\n",
    "\n",
    "# Generate recommendations\n",
    "def generate_recommendations(algo, df, user_id, n_recommendations, item_features, item_popularity, global_popularity, verbose=False):\n",
    "    \"\"\"Generate top N recommendations for a given user with diversity penalty.\"\"\"\n",
    "    user_rated_items = df[df['userId'] == user_id]['movieId'].tolist()\n",
    "    all_items = df['movieId'].unique()\n",
    "    recommendations = []\n",
    "\n",
    "    cf_scores = []\n",
    "    cbf_scores = []\n",
    "    items_to_recommend = []\n",
    "\n",
    "    for item_id in all_items:\n",
    "        if item_id not in user_rated_items:\n",
    "            pred = algo.predict(user_id, item_id)\n",
    "            cf_scores.append(pred.est)\n",
    "            content_score = np.mean(list(item_features.get(item_id, {}).values()))\n",
    "            cbf_scores.append(content_score)\n",
    "            items_to_recommend.append(item_id)\n",
    "\n",
    "    normalized_cbf_scores = normalize_scores(cbf_scores)\n",
    "\n",
    "    for idx, item_id in enumerate(items_to_recommend):\n",
    "        cf_score = cf_scores[idx]\n",
    "        content_score = normalized_cbf_scores[idx]\n",
    "        popularity_penalty = item_popularity.get(item_id, 0) / len(df['userId'].unique())\n",
    "        diversity_penalty = global_popularity.get(item_id, 0) / len(df['userId'].unique())\n",
    "        hybrid_score = (CF_WEIGHT * cf_score + CBF_WEIGHT * content_score) - (popularity_penalty * 0.01) - (diversity_penalty * 0.01)\n",
    "        if verbose:\n",
    "            print(f\"User {user_id}, Item {item_id}: CF {cf_score:.2f}, CBF {content_score:.2f}, Penalty {popularity_penalty:.4f}, Diversity {diversity_penalty:.4f}, Hybrid {hybrid_score:.2f}\")\n",
    "        recommendations.append((item_id, hybrid_score))\n",
    "\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n_recommendations]\n",
    "\n",
    "# Calculate item popularity\n",
    "def calculate_item_popularity(df):\n",
    "    \"\"\"Calculate how often each item is rated.\"\"\"\n",
    "    item_popularity = df['movieId'].value_counts().to_dict()\n",
    "    return item_popularity\n",
    "\n",
    "# Expected recommendations for novelty\n",
    "def get_expected_recommendations(df):\n",
    "    \"\"\"Get expected recommendations based on high ratings.\"\"\"\n",
    "    return df[df['rating'] >= RATING_THRESHOLD]['movieId'].unique()\n",
    "\n",
    "# Calculate model metrics\n",
    "def calculate_model_metrics(predictions):\n",
    "    \"\"\"Calculate various metrics to evaluate the model.\"\"\"\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    estimated_ratings = [pred.est for pred in predictions]\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "    y_true = [1 if true_r >= RATING_THRESHOLD else 0 for true_r in true_ratings]\n",
    "    y_pred = [1 if est >= RATING_THRESHOLD else 0 for est in estimated_ratings]\n",
    "    precision = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_pred)\n",
    "    recall = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_true)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Calculate hit rate\n",
    "def calculate_hit_rate(recommendations, df, n_recommendations):\n",
    "    \"\"\"Calculate hit rate.\"\"\"\n",
    "    hits = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        hits += len(set(item_id for item_id, _ in items).intersection(set(user_rated_items)))\n",
    "    return hits / (len(recommendations) * n_recommendations)\n",
    "\n",
    "# Calculate mean reciprocal rank\n",
    "def calculate_mrr(recommendations, df):\n",
    "    \"\"\"Calculate mean reciprocal rank.\"\"\"\n",
    "    rr_sum = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        for rank, (item, _) in enumerate(items, start=1):\n",
    "            if item in user_rated_items:\n",
    "                rr_sum += 1 / rank\n",
    "                break\n",
    "    return rr_sum / len(recommendations)\n",
    "\n",
    "# Calculate average precision\n",
    "def calculate_ap(recommendations, df):\n",
    "    \"\"\"Calculate average precision.\"\"\"\n",
    "    ap_sum = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        relevant_items = set(user_rated_items)\n",
    "        hits = 0\n",
    "        precision_sum = 0\n",
    "        for rank, (item, _) in enumerate(items, start=1):\n",
    "            if item in relevant_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / rank\n",
    "        ap_sum += precision_sum / min(len(relevant_items), len(items))\n",
    "    return ap_sum / len(recommendations)\n",
    "\n",
    "# Calculate coverage\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    \"\"\"Calculate the percentage of items in the catalog that have been recommended.\"\"\"\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item_id for item_id, _ in items])\n",
    "    return len(recommended_items) / catalog_size\n",
    "\n",
    "# Calculate novelty\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    \"\"\"Calculate the average popularity of recommended items.\"\"\"\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            novelty_scores.append(item_popularity.get(item_id, 0))\n",
    "    return np.mean(novelty_scores)\n",
    "\n",
    "# Calculate personalization\n",
    "def calculate_personalization(recommendations):\n",
    "    \"\"\"Calculate how different the recommendations are for different users.\"\"\"\n",
    "    user_pairs = list(itertools.combinations(recommendations.keys(), 2))\n",
    "    similarity_sum = 0\n",
    "    for user1, user2 in user_pairs:\n",
    "        items1 = {item_id for item_id, _ in recommendations[user1]}\n",
    "        items2 = {item_id for item_id, _ in recommendations[user2]}\n",
    "        similarity_sum += len(items1 & items2) / len(items1 | items2)\n",
    "    return 1 - (similarity_sum / len(user_pairs))\n",
    "\n",
    "# Calculate serendipity\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features):\n",
    "    \"\"\"Calculate the serendipity of the recommendations.\"\"\"\n",
    "    serendipity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        user_prefs = user_preferences.get(user, {})\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_recommendations:\n",
    "                item_genres = item_features.get(item_id, {})\n",
    "                similarity = sum(user_prefs.get(f\"user_mean_{genre}\", 0) * item_genres.get(genre, 0) for genre in item_genres)\n",
    "                serendipity_scores.append(1 - similarity)\n",
    "    return np.mean(serendipity_scores) if serendipity_scores else 0\n",
    "\n",
    "# Calculate intra-list diversity\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    \"\"\"Calculate the diversity within a single user's list of recommendations.\"\"\"\n",
    "    diversity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        features = [list(item_features[item[0]].values()) for item in items if item[0] in item_features]\n",
    "        if len(features) > 1:\n",
    "            distances = pdist(features, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(distances))\n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "# Display metrics\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features, df, n_recommendations):\n",
    "    \"\"\"Display various metrics to evaluate the recommendations.\"\"\"\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "    hit_rate = calculate_hit_rate(recommendations, df, n_recommendations)\n",
    "    mrr = calculate_mrr(recommendations, df)\n",
    "    ap = calculate_ap(recommendations, df)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.4f}\")\n",
    "    print(f\"Serendipity: {serendipity:.4f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.4f}\")\n",
    "    print(f\"Hit Rate: {hit_rate:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "    print(f\"Average Precision (AP): {ap:.4f}\")\n",
    "\n",
    "# Function to calculate metrics on recommended movies\n",
    "def calculate_recommendation_metrics(recommendations, df):\n",
    "    \"\"\"Calculate metrics (RMSE, MAE, Precision, Recall, F1) on the recommended movies.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user][['movieId', 'rating']]\n",
    "        for item_id, predicted_rating in items:\n",
    "            actual_rating = user_rated_items[user_rated_items['movieId'] == item_id]['rating'].values\n",
    "            if len(actual_rating) > 0:\n",
    "                y_true.append(actual_rating[0])\n",
    "                y_pred.append(predicted_rating)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    precision = precision_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    recall = recall_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    f1 = f1_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data and prepare for EDA.\"\"\"\n",
    "    merged_df, movies_df, ratings_df = load_data(MOVIES_FILE, RATINGS_FILE, user_sample_size=USER_SAMPLE_SIZE)\n",
    "    if merged_df is not None:\n",
    "        print(\"Data loaded successfully. You can now perform EDA on the loaded datasets.\")\n",
    "        return merged_df, movies_df, ratings_df\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "        return None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_df, movies_df, ratings_df = main()\n",
    "\n",
    "    # Example EDA\n",
    "    if merged_df is not None:\n",
    "        print(\"\\nMovies DataFrame:\")\n",
    "        print(movies_df.head())\n",
    "        \n",
    "        print(\"\\nRatings DataFrame:\")\n",
    "        print(ratings_df.head())\n",
    "        \n",
    "        print(\"\\nMerged DataFrame:\")\n",
    "        print(merged_df.head())\n",
    "        \n",
    "        # Example EDA: Distribution of ratings\n",
    "        ratings_distribution = ratings_df['rating'].value_counts().sort_index()\n",
    "        print(\"\\nRatings Distribution:\")\n",
    "        print(ratings_distribution)\n",
    "        \n",
    "        # Example EDA: Number of ratings per user\n",
    "        user_ratings_count = ratings_df['userId'].value_counts()\n",
    "        print(\"\\nNumber of Ratings per User:\")\n",
    "        print(user_ratings_count.describe())\n",
    "        \n",
    "        # Train the model and generate recommendations\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(merged_df[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        # Hyperparameter tuning with GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_factors': [50, 100],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.1],\n",
    "            'biased': [True, False]  # Adding the 'biased' parameter\n",
    "        }\n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)  # Use all available CPU cores\n",
    "        gs.fit(data)\n",
    "        best_params = gs.best_params['rmse']\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        # Train the best model\n",
    "        algo = SVD(**best_params)\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions = algo.test(testset)\n",
    "        metrics = calculate_model_metrics(predictions)\n",
    "        print(\"Model Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Generate recommendations for all users\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "        item_popularity = calculate_item_popularity(merged_df)\n",
    "        global_popularity = item_popularity  # Assuming global popularity is the same as item popularity here\n",
    "        recommendations = {user_id: generate_recommendations(algo, merged_df, user_id, N_RECOMMENDATIONS, item_features, item_popularity, global_popularity) for user_id in merged_df['userId'].unique()}\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "\n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features, merged_df, N_RECOMMENDATIONS)\n",
    "\n",
    "        # Calculate and display recommendation metrics\n",
    "        recommendation_metrics = calculate_recommendation_metrics(recommendations, merged_df)\n",
    "        print(\"Recommendation Metrics:\")\n",
    "        for metric, value in recommendation_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Print recommendations for the first 10 users\n",
    "        for user_id, recs in list(recommendations.items())[:10]:\n",
    "            print(f\"User {user_id} recommendations:\")\n",
    "            for movie_id, est_rating in recs:\n",
    "                title = movies_df[movies_df['movieId'] == movie_id]['title'].values[0]\n",
    "                print(f\"  {title}: {est_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1410f153-db09-428f-9aa0-2dc23135e0bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4dbcc0a-9446-45e3-9f21-bee8c731dc73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9568ec0-ca7f-40c5-a50c-f7eb1da5f56b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4880794-756e-4c2c-bae4-1e38f64f04de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "811de244-9b20-4d51-a94b-62d7de3908c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the details of your preferred movie profile.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Do you like Documentary movies? (yes/no):  no\n",
      "Do you like Action movies? (yes/no):  y\n",
      "Do you like (no genres listed) movies? (yes/no):  n\n",
      "Do you like Film-Noir movies? (yes/no):  n\n",
      "Do you like Fantasy movies? (yes/no):  y\n",
      "Do you like IMAX movies? (yes/no):  y\n",
      "Do you like Mystery movies? (yes/no):  n\n",
      "Do you like Drama movies? (yes/no):  y\n",
      "Do you like Romance movies? (yes/no):  n\n",
      "Do you like Adventure movies? (yes/no):  y\n",
      "Do you like Crime movies? (yes/no):  n\n",
      "Do you like Western movies? (yes/no):  n\n",
      "Do you like Musical movies? (yes/no):  n\n",
      "Do you like Thriller movies? (yes/no):  y\n",
      "Do you like Sci-Fi movies? (yes/no):  y\n",
      "Do you like Children movies? (yes/no):  n\n",
      "Do you like War movies? (yes/no):  y\n",
      "Do you like Comedy movies? (yes/no):  y\n",
      "Do you like Animation movies? (yes/no):  n\n",
      "Do you like Horror movies? (yes/no):  y\n",
      "Enter the preferred release year (e.g., 2000):  2000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (21,) and (23,) not aligned: 21 (dim 0) != 23 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 38\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitle\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Example usage (run this after the main function):\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m recommend_for_profile(algo, merged_df, movies_df, item_features, item_popularity)\n",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36mrecommend_for_profile\u001b[1;34m(algo, merged_df, movies_df, item_features, item_popularity)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item_id \u001b[38;5;129;01min\u001b[39;00m item_features\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     23\u001b[0m     item_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(item_features[item_id]\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 24\u001b[0m     similarity \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(profile_vector, item_vector) \u001b[38;5;241m/\u001b[39m (np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(profile_vector) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(item_vector))\n\u001b[0;32m     25\u001b[0m     content_scores\u001b[38;5;241m.\u001b[39mappend((item_id, similarity))\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Normalize content scores\u001b[39;00m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (21,) and (23,) not aligned: 21 (dim 0) != 23 (dim 0)"
     ]
    }
   ],
   "source": [
    "def recommend_for_profile(algo, merged_df, movies_df, item_features, item_popularity):\n",
    "    \"\"\"Prompt the user for a movie profile and generate recommendations.\"\"\"\n",
    "    print(\"Please enter the details of your preferred movie profile.\")\n",
    "    \n",
    "    # Get genres from the user\n",
    "    genres = []\n",
    "    available_genres = list(set(itertools.chain.from_iterable(merged_df['genres'].str.split('|'))))\n",
    "    for genre in available_genres:\n",
    "        include = input(f\"Do you like {genre} movies? (yes/no): \").strip().lower()\n",
    "        if include == 'yes':\n",
    "            genres.append(genre)\n",
    "    \n",
    "    # Get release year from the user\n",
    "    release_year = float(input(\"Enter the preferred release year (e.g., 2000): \"))\n",
    "    weighted_release_year = get_weighted_release_year(release_year, YEAR_DIVISOR)\n",
    "    \n",
    "    # Create a profile vector\n",
    "    profile_vector = [weighted_release_year] + [1 if genre in genres else 0 for genre in available_genres]\n",
    "    \n",
    "    # Calculate content scores for all items\n",
    "    content_scores = []\n",
    "    for item_id in item_features.keys():\n",
    "        item_vector = list(item_features[item_id].values())\n",
    "        similarity = np.dot(profile_vector, item_vector) / (np.linalg.norm(profile_vector) * np.linalg.norm(item_vector))\n",
    "        content_scores.append((item_id, similarity))\n",
    "    \n",
    "    # Normalize content scores\n",
    "    content_scores = sorted(content_scores, key=lambda x: x[1], reverse=True)\n",
    "    content_scores = content_scores[:N_RECOMMENDATIONS]\n",
    "    \n",
    "    # Print recommendations\n",
    "    print(\"\\nRecommended movies based on your profile:\")\n",
    "    for item_id, score in content_scores:\n",
    "        title = movies_df[movies_df['movieId'] == item_id]['title'].values[0]\n",
    "        print(f\"{title}: {score:.2f}\")\n",
    "\n",
    "# Example usage (run this after the main function):\n",
    "recommend_for_profile(algo, merged_df, movies_df, item_features, item_popularity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82639c3-3182-4330-80f0-b6024596c358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a027a7ae-2bea-444d-b71d-ddcb512ea882",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import logging\n",
    "import re\n",
    "from surprise import Dataset, Reader, SVD\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "\n",
    "# Constants\n",
    "MOVIES_FILE = '../data/movies.csv'\n",
    "RATINGS_FILE = '../data/ratings.csv'\n",
    "N_RECOMMENDATIONS = 5\n",
    "YEAR_DIVISOR = 1.0  # Adjust this value based on experimentation\n",
    "RATING_THRESHOLD = 4.0\n",
    "RANDOM_SEED = 42  # Random seed for reproducibility\n",
    "USER_SAMPLE_SIZE = 100  # Set sample size to 100 for now\n",
    "\n",
    "# Weights for hybrid scoring\n",
    "CF_WEIGHT = 0.7\n",
    "CBF_WEIGHT = 0.3\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Load datasets\n",
    "def load_data(movies_file, ratings_file, user_sample_size=None):\n",
    "    \"\"\"Load movies and ratings datasets and merge them. Optionally sample a subset of users.\"\"\"\n",
    "    try:\n",
    "        movies_df = pd.read_csv(movies_file)\n",
    "        ratings_df = pd.read_csv(ratings_file)\n",
    "        \n",
    "        if user_sample_size:\n",
    "            unique_users = ratings_df['userId'].drop_duplicates()\n",
    "            actual_sample_size = min(user_sample_size, len(unique_users))\n",
    "            sampled_users = unique_users.sample(n=actual_sample_size, random_state=RANDOM_SEED)\n",
    "            ratings_df = ratings_df[ratings_df['userId'].isin(sampled_users)]\n",
    "        \n",
    "        merged_df = pd.merge(ratings_df, movies_df, on='movieId')\n",
    "        if 'release_year' not in merged_df.columns:\n",
    "            merged_df['release_year'] = merged_df['title'].str.extract(r'\\((\\d{4})\\)')[0].astype(float)\n",
    "        \n",
    "        # Ensure proper genre handling\n",
    "        genre_list = list(set(itertools.chain.from_iterable(merged_df['genres'].str.split('|'))))\n",
    "        for genre in genre_list:\n",
    "            genre_pattern = re.escape(genre)  # Escape genre to treat it as a literal string\n",
    "            merged_df[genre] = merged_df['genres'].str.contains(r'\\b' + genre_pattern + r'\\b').astype(int)\n",
    "        return merged_df, movies_df, ratings_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "# Calculate weighted release year\n",
    "def get_weighted_release_year(year, divisor):\n",
    "    \"\"\"Calculate the weighted release year by normalizing and then dividing the year by the given divisor.\"\"\"\n",
    "    min_year = 1900  # Assuming movies are not older than 1900\n",
    "    max_year = 2024  # Use the current year as the upper bound\n",
    "    normalized_year = (year - min_year) / (max_year - min_year)\n",
    "    return normalized_year / divisor\n",
    "\n",
    "# Get item features\n",
    "def get_item_features(df):\n",
    "    \"\"\"Extract item features from the DataFrame.\"\"\"\n",
    "    if 'release_year' not in df.columns:\n",
    "        raise KeyError(\"release_year column is missing from the DataFrame.\")\n",
    "    df['release_year_bucket'] = df['release_year'].apply(lambda x: get_weighted_release_year(x, YEAR_DIVISOR))\n",
    "    genre_columns = [col for col in df.columns if col not in ['userId', 'movieId', 'rating', 'title', 'genres', 'release_year_bucket']]\n",
    "    item_features = df[['movieId', 'release_year_bucket'] + genre_columns].drop_duplicates().set_index('movieId')\n",
    "    # Ensure index is unique\n",
    "    item_features = item_features.loc[~item_features.index.duplicated(keep='first')]\n",
    "    # Handle missing values by filling them with zeros or an appropriate value\n",
    "    item_features = item_features.fillna(0)\n",
    "    # Debugging: Print item features to ensure correctness\n",
    "    print(\"Item features:\")\n",
    "    print(item_features.head())\n",
    "    item_features_dict = item_features.to_dict(orient='index')\n",
    "    return item_features_dict\n",
    "\n",
    "# Derive user preferences\n",
    "def derive_user_preferences(df):\n",
    "    \"\"\"Calculate mean ratings for each genre and release year for each user.\"\"\"\n",
    "    numeric_columns = df.select_dtypes(include=[np.number]).columns\n",
    "    user_genre_means = df.groupby('userId')[numeric_columns].mean().add_prefix('user_mean_')\n",
    "    return user_genre_means.to_dict(orient='index')\n",
    "\n",
    "# Normalize content scores\n",
    "def normalize_scores(scores):\n",
    "    \"\"\"Normalize scores to be between 0.5 and 5.0.\"\"\"\n",
    "    scores = np.array(scores)\n",
    "    min_score = np.nanmin(scores)\n",
    "    max_score = np.nanmax(scores)\n",
    "    if max_score != min_score:\n",
    "        normalized_scores = 0.5 + 4.5 * ((scores - min_score) / (max_score - min_score))\n",
    "    else:\n",
    "        normalized_scores = scores\n",
    "    return normalized_scores\n",
    "\n",
    "# Generate recommendations\n",
    "def generate_recommendations(algo, df, user_id, n_recommendations, item_features, item_popularity, verbose=False):\n",
    "    \"\"\"Generate top N recommendations for a given user.\"\"\"\n",
    "    user_rated_items = df[df['userId'] == user_id]['movieId'].tolist()\n",
    "    all_items = df['movieId'].unique()\n",
    "    recommendations = []\n",
    "\n",
    "    cf_scores = []\n",
    "    cbf_scores = []\n",
    "    items_to_recommend = []\n",
    "\n",
    "    for item_id in all_items:\n",
    "        if item_id not in user_rated_items:\n",
    "            pred = algo.predict(user_id, item_id)\n",
    "            cf_scores.append(pred.est)\n",
    "            content_score = np.mean(list(item_features.get(item_id, {}).values()))\n",
    "            cbf_scores.append(content_score)\n",
    "            items_to_recommend.append(item_id)\n",
    "\n",
    "    normalized_cbf_scores = normalize_scores(cbf_scores)\n",
    "\n",
    "    for idx, item_id in enumerate(items_to_recommend):\n",
    "        cf_score = cf_scores[idx]\n",
    "        content_score = normalized_cbf_scores[idx]\n",
    "        popularity_penalty = item_popularity.get(item_id, 0) / len(df['userId'].unique())\n",
    "        hybrid_score = (CF_WEIGHT * cf_score + CBF_WEIGHT * content_score) - (popularity_penalty * 0.01)\n",
    "        if verbose:\n",
    "            print(f\"User {user_id}, Item {item_id}: CF {cf_score:.2f}, CBF {content_score:.2f}, Penalty {popularity_penalty:.4f}, Hybrid {hybrid_score:.2f}\")\n",
    "        recommendations.append((item_id, hybrid_score))\n",
    "\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n_recommendations]\n",
    "\n",
    "# Calculate item popularity\n",
    "def calculate_item_popularity(df):\n",
    "    \"\"\"Calculate how often each item is rated.\"\"\"\n",
    "    item_popularity = df['movieId'].value_counts().to_dict()\n",
    "    return item_popularity\n",
    "\n",
    "# Expected recommendations for novelty\n",
    "def get_expected_recommendations(df):\n",
    "    \"\"\"Get expected recommendations based on high ratings.\"\"\"\n",
    "    return df[df['rating'] >= RATING_THRESHOLD]['movieId'].unique()\n",
    "\n",
    "# Calculate model metrics\n",
    "def calculate_model_metrics(predictions):\n",
    "    \"\"\"Calculate various metrics to evaluate the model.\"\"\"\n",
    "    true_ratings = [pred.r_ui for pred in predictions]\n",
    "    estimated_ratings = [pred.est for pred in predictions]\n",
    "    rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "    mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "    y_true = [1 if true_r >= RATING_THRESHOLD else 0 for true_r in true_ratings]\n",
    "    y_pred = [1 if est >= RATING_THRESHOLD else 0 for est in estimated_ratings]\n",
    "    precision = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_pred)\n",
    "    recall = np.sum(np.array(y_true) & np.array(y_pred)) / np.sum(y_true)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Calculate hit rate\n",
    "def calculate_hit_rate(recommendations, df, n_recommendations):\n",
    "    \"\"\"Calculate hit rate.\"\"\"\n",
    "    hits = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        hits += len(set(item_id for item_id, _ in items).intersection(set(user_rated_items)))\n",
    "    return hits / (len(recommendations) * n_recommendations)\n",
    "\n",
    "# Calculate mean reciprocal rank\n",
    "def calculate_mrr(recommendations, df):\n",
    "    \"\"\"Calculate mean reciprocal rank.\"\"\"\n",
    "    rr_sum = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        for rank, (item, _) in enumerate(items, start=1):\n",
    "            if item in user_rated_items:\n",
    "                rr_sum += 1 / rank\n",
    "                break\n",
    "    return rr_sum / len(recommendations)\n",
    "\n",
    "# Calculate average precision\n",
    "def calculate_ap(recommendations, df):\n",
    "    \"\"\"Calculate average precision.\"\"\"\n",
    "    ap_sum = 0\n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user]['movieId'].tolist()\n",
    "        relevant_items = set(user_rated_items)\n",
    "        hits = 0\n",
    "        precision_sum = 0\n",
    "        for rank, (item, _) in enumerate(items, start=1):\n",
    "            if item in relevant_items:\n",
    "                hits += 1\n",
    "                precision_sum += hits / rank\n",
    "        ap_sum += precision_sum / min(len(relevant_items), len(items))\n",
    "    return ap_sum / len(recommendations)\n",
    "\n",
    "# Calculate coverage\n",
    "def calculate_coverage(recommendations, catalog_size):\n",
    "    \"\"\"Calculate the percentage of items in the catalog that have been recommended.\"\"\"\n",
    "    recommended_items = set()\n",
    "    for user, items in recommendations.items():\n",
    "        recommended_items.update([item_id for item_id, _ in items])\n",
    "    return len(recommended_items) / catalog_size\n",
    "\n",
    "# Calculate novelty\n",
    "def calculate_novelty(recommendations, item_popularity):\n",
    "    \"\"\"Calculate the average popularity of recommended items.\"\"\"\n",
    "    novelty_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        for item_id, _ in items:\n",
    "            novelty_scores.append(item_popularity.get(item_id, 0))\n",
    "    return np.mean(novelty_scores)\n",
    "\n",
    "# Calculate personalization\n",
    "def calculate_personalization(recommendations):\n",
    "    \"\"\"Calculate how different the recommendations are for different users.\"\"\"\n",
    "    user_pairs = list(itertools.combinations(recommendations.keys(), 2))\n",
    "    similarity_sum = 0\n",
    "    for user1, user2 in user_pairs:\n",
    "        items1 = {item_id for item_id, _ in recommendations[user1]}\n",
    "        items2 = {item_id for item_id, _ in recommendations[user2]}\n",
    "        similarity_sum += len(items1 & items2) / len(items1 | items2)\n",
    "    return 1 - (similarity_sum / len(user_pairs))\n",
    "\n",
    "# Calculate serendipity\n",
    "def calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features):\n",
    "    \"\"\"Calculate the serendipity of the recommendations.\"\"\"\n",
    "    serendipity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        user_prefs = user_preferences.get(user, {})\n",
    "        for item_id, _ in items:\n",
    "            if item_id not in expected_recommendations:\n",
    "                item_genres = item_features.get(item_id, {})\n",
    "                similarity = sum(user_prefs.get(f\"user_mean_{genre}\", 0) * item_genres.get(genre, 0) for genre in item_genres)\n",
    "                serendipity_scores.append(1 - similarity)\n",
    "    return np.mean(serendipity_scores) if serendipity_scores else 0\n",
    "\n",
    "# Calculate intra-list diversity\n",
    "def calculate_intra_list_similarity(recommendations, item_features):\n",
    "    \"\"\"Calculate the diversity within a single user's list of recommendations.\"\"\"\n",
    "    diversity_scores = []\n",
    "    for user, items in recommendations.items():\n",
    "        features = [list(item_features[item[0]].values()) for item in items if item[0] in item_features]\n",
    "        if len(features) > 1:\n",
    "            distances = pdist(features, 'cosine')\n",
    "            diversity_scores.append(1 - np.mean(distances))\n",
    "    return np.mean(diversity_scores) if diversity_scores else 0\n",
    "\n",
    "# Display metrics\n",
    "def display_metrics(recommendations, catalog_size, item_popularity, expected_recommendations, user_preferences, item_features, df, n_recommendations):\n",
    "    \"\"\"Display various metrics to evaluate the recommendations.\"\"\"\n",
    "    coverage = calculate_coverage(recommendations, catalog_size)\n",
    "    novelty = calculate_novelty(recommendations, item_popularity)\n",
    "    personalization = calculate_personalization(recommendations)\n",
    "    serendipity = calculate_serendipity(recommendations, expected_recommendations, user_preferences, item_features)\n",
    "    intra_list_diversity = calculate_intra_list_similarity(recommendations, item_features)\n",
    "    hit_rate = calculate_hit_rate(recommendations, df, n_recommendations)\n",
    "    mrr = calculate_mrr(recommendations, df)\n",
    "    ap = calculate_ap(recommendations, df)\n",
    "\n",
    "    print(f\"Catalog Coverage: {coverage:.2%}\")\n",
    "    print(f\"Average Novelty: {novelty:.4f}\")\n",
    "    print(f\"Personalization: {personalization:.4f}\")\n",
    "    print(f\"Serendipity: {serendipity:.4f}\")\n",
    "    print(f\"Intra-list Diversity: {intra_list_diversity:.4f}\")\n",
    "    print(f\"Hit Rate: {hit_rate:.4f}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR): {mrr:.4f}\")\n",
    "    print(f\"Average Precision (AP): {ap:.4f}\")\n",
    "\n",
    "# Function to calculate metrics on recommended movies\n",
    "def calculate_recommendation_metrics(recommendations, df):\n",
    "    \"\"\"Calculate metrics (RMSE, MAE, Precision, Recall, F1) on the recommended movies.\"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    for user, items in recommendations.items():\n",
    "        user_rated_items = df[df['userId'] == user][['movieId', 'rating']]\n",
    "        for item_id, predicted_rating in items:\n",
    "            actual_rating = user_rated_items[user_rated_items['movieId'] == item_id]['rating'].values\n",
    "            if len(actual_rating) > 0:\n",
    "                y_true.append(actual_rating[0])\n",
    "                y_pred.append(predicted_rating)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    precision = precision_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    recall = recall_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    f1 = f1_score([1 if y >= RATING_THRESHOLD else 0 for y in y_true], [1 if y >= RATING_THRESHOLD else 0 for y in y_pred], zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to load data and prepare for EDA.\"\"\"\n",
    "    merged_df, movies_df, ratings_df = load_data(MOVIES_FILE, RATINGS_FILE, user_sample_size=USER_SAMPLE_SIZE)\n",
    "    if merged_df is not None:\n",
    "        print(\"Data loaded successfully. You can now perform EDA on the loaded datasets.\")\n",
    "        return merged_df, movies_df, ratings_df\n",
    "    else:\n",
    "        print(\"Data loading or processing failed.\")\n",
    "        return None, None, None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    merged_df, movies_df, ratings_df = main()\n",
    "\n",
    "    # Example EDA\n",
    "    if merged_df is not None:\n",
    "        print(\"\\nMovies DataFrame:\")\n",
    "        print(movies_df.head())\n",
    "        \n",
    "        print(\"\\nRatings DataFrame:\")\n",
    "        print(ratings_df.head())\n",
    "        \n",
    "        print(\"\\nMerged DataFrame:\")\n",
    "        print(merged_df.head())\n",
    "        \n",
    "        # Example EDA: Distribution of ratings\n",
    "        ratings_distribution = ratings_df['rating'].value_counts().sort_index()\n",
    "        print(\"\\nRatings Distribution:\")\n",
    "        print(ratings_distribution)\n",
    "        \n",
    "        # Example EDA: Number of ratings per user\n",
    "        user_ratings_count = ratings_df['userId'].value_counts()\n",
    "        print(\"\\nNumber of Ratings per User:\")\n",
    "        print(user_ratings_count.describe())\n",
    "        \n",
    "        # Train the model and generate recommendations\n",
    "        reader = Reader(rating_scale=(0.5, 5.0))\n",
    "        data = Dataset.load_from_df(merged_df[['userId', 'movieId', 'rating']], reader)\n",
    "        trainset, testset = train_test_split(data, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "        # Hyperparameter tuning with GridSearchCV\n",
    "        param_grid = {\n",
    "            'n_factors': [50, 100],\n",
    "            'n_epochs': [20, 30],\n",
    "            'lr_all': [0.005, 0.01],\n",
    "            'reg_all': [0.02, 0.1],\n",
    "            'biased': [True, False]  # Adding the 'biased' parameter\n",
    "        }\n",
    "        gs = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=3, n_jobs=-1)  # Use all available CPU cores\n",
    "        gs.fit(data)\n",
    "        best_params = gs.best_params['rmse']\n",
    "        print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "        # Train the best model\n",
    "        algo = SVD(**best_params)\n",
    "        algo.fit(trainset)\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions = algo.test(testset)\n",
    "        metrics = calculate_model_metrics(predictions)\n",
    "        print(\"Model Metrics:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Generate recommendations for all users\n",
    "        item_features = get_item_features(merged_df)\n",
    "        user_preferences = derive_user_preferences(merged_df)\n",
    "        item_popularity = calculate_item_popularity(merged_df)\n",
    "        recommendations = {user_id: generate_recommendations(algo, merged_df, user_id, N_RECOMMENDATIONS, item_features, item_popularity) for user_id in merged_df['userId'].unique()}\n",
    "        expected_recommendations = get_expected_recommendations(merged_df)\n",
    "\n",
    "        display_metrics(recommendations, len(movies_df), item_popularity, expected_recommendations, user_preferences, item_features, merged_df, N_RECOMMENDATIONS)\n",
    "\n",
    "        # Calculate and display recommendation metrics\n",
    "        recommendation_metrics = calculate_recommendation_metrics(recommendations, merged_df)\n",
    "        print(\"Recommendation Metrics:\")\n",
    "        for metric, value in recommendation_metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "        # Print recommendations for the first 10 users\n",
    "        for user_id, recs in list(recommendations.items())[:10]:\n",
    "            print(f\"User {user_id} recommendations:\")\n",
    "            for movie_id, est_rating in recs:\n",
    "                title = movies_df[movies_df['movieId'] == movie_id]['title'].values[0]\n",
    "                print(f\"  {title}: {est_rating:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d98ff2a-1a5f-41cf-bb8e-0951948fbb1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6347469d-d93e-4bf3-9ec2-fa554b59fa63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b29c57-fc87-44b9-b6f2-f9b817de5a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c66be491-3436-4dfc-9bbd-0239b3d0150b",
   "metadata": {},
   "source": [
    "Explanation of Key Functions:\n",
    "\n",
    "- load_data: Loads and merges the movie and rating datasets, extracting the release year if necessary.\n",
    "- get_weighted_release_year: Computes the weighted release year by dividing the year by a divisor.\n",
    "- get_item_features: Extracts item features from the DataFrame, ensuring the release year is included.\n",
    "- derive_user_preferences: Calculates mean ratings for each genre and release year for each user, focusing only on numeric columns.\n",
    "- generate_recommendations: Generates top N recommendations for a given user, excluding already rated items.\n",
    "- calculate_item_popularity: Computes how often each item is recommended across all users.\n",
    "- get_expected_recommendations: Identifies items that are expected to be recommended based on high ratings.\n",
    "- calculate_model_metrics: Computes RMSE, MAE, precision, recall, and F1 score to evaluate the model.\n",
    "- calculate_coverage: Determines the percentage of items in the catalog that have been recommended.\n",
    "- calculate_novelty: Calculates the average popularity of recommended items.\n",
    "- calculate_personalization: Measures how different the recommendations are for different users.\n",
    "- calculate_serendipity: Evaluates the serendipity of the recommendations.\n",
    "- calculate_intra_list_similarity: Assesses the diversity within a single user's list of recommendations.\n",
    "- display_metrics: Displays various metrics to evaluate the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae89e0-2fdf-4040-a94d-47cdb1ad46d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb49e6c-f2d5-40a5-bd16-f065a6f617a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from surprise.accuracy import rmse, mae\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, precision_score, recall_score, f1_score\n",
    "import random\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use ggplto style for graphs\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette('colorblind')\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# Your function definitions here...\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52588b8f-07b1-4580-a42e-9e0cbb591240",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 2: Compute and Plot Metrics at Different Values of K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521abd94-577c-4b98-b349-decf32a6f053",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_top_k(predictions, k_values):\n",
    "    metrics = {\n",
    "        'k': [],\n",
    "        'precision': [],\n",
    "        'recall': [],\n",
    "        'f1': [],\n",
    "        'rmse': [],\n",
    "        'mae': []\n",
    "    }\n",
    "    \n",
    "    for k in k_values:\n",
    "        top_k_preds = defaultdict(list)\n",
    "        for uid, _, true_r, est, _ in predictions:\n",
    "            top_k_preds[uid].append((est, true_r))\n",
    "        \n",
    "        for uid in top_k_preds:\n",
    "            top_k_preds[uid].sort(reverse=True, key=lambda x: x[0])\n",
    "            top_k_preds[uid] = top_k_preds[uid][:k]\n",
    "        \n",
    "        flat_preds = [pred for sublist in top_k_preds.values() for pred in sublist]\n",
    "        estimated_ratings = [pred[0] for pred in flat_preds]\n",
    "        true_ratings = [pred[1] for pred in flat_preds]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(true_ratings, estimated_ratings))\n",
    "        mae = mean_absolute_error(true_ratings, estimated_ratings)\n",
    "        y_true = [1 if r >= THRESHOLD else 0 for r in true_ratings]\n",
    "        y_pred = [1 if r >= THRESHOLD else 0 for r in estimated_ratings]\n",
    "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "        \n",
    "        metrics['k'].append(k)\n",
    "        metrics['precision'].append(precision)\n",
    "        metrics['recall'].append(recall)\n",
    "        metrics['f1'].append(f1)\n",
    "        metrics['rmse'].append(rmse)\n",
    "        metrics['mae'].append(mae)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def plot_metrics(metrics):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(metrics['k'], metrics['precision'], marker='o', linestyle='-')\n",
    "    plt.title('Precision at top k')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(metrics['k'], metrics['recall'], marker='o', linestyle='-')\n",
    "    plt.title('Recall at top k')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Recall')\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(metrics['k'], metrics['f1'], marker='o', linestyle='-')\n",
    "    plt.title('F1-Score at top k')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('F1 Score')\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(metrics['k'], metrics['rmse'], marker='o', linestyle='-')\n",
    "    plt.title('RMSE at top k')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('RMSE')\n",
    "    \n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(metrics['k'], metrics['mae'], marker='o', linestyle='-')\n",
    "    plt.title('MAE at top k')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('MAE')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7331ad7-9bcd-4776-8e91-436f55243028",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Section 3: Print Single Evaluation Metrics - Evaluation against Industry Avereges\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026c88b1-5fdd-4ef0-99e5-7d3b8e3b6d89",
   "metadata": {},
   "source": [
    "Typical Accuracy Levels:\n",
    "\n",
    "- General Accuracy: It's challenging to state a specific \"average accuracy\" because it depends highly on the context and the specific system configuration. However, good movie recommendation systems generally achieve:\n",
    "    - RMSE: Values around 0.8 to 1.2 for rating predictions, with lower values indicating better accuracy.\n",
    "    - Precision/Recall: Precision and recall can vary, but good systems might achieve over 20-30% precision in top-N recommendations in practical settings.\n",
    "    - High-Performance Systems: In competitions like the Netflix Prize, the winning entries achieved RMSEs around 0.85, considered very high accuracy in a real-world system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9ef72f-b9ad-478c-be97-0e4ca238a250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "# Ranges for industry standards (min and max)\n",
    "industry_ranges = {\n",
    "    'RMSE': (0.85, 0.95),       # Min and max RMSE in industry\n",
    "    'MAE': (0.70, 0.75),         # Min and max MAE in industry\n",
    "    'Precision': (0.70, 0.80),  # Min and max precision in industry\n",
    "    'Recall': (0.10, 0.40),     # Min and max recall in industry\n",
    "    'F1 Score': (0.30, 0.50),   # Min and max F1 score in industry\n",
    "}\n",
    "\n",
    "metrics = list(model_metrics.keys())\n",
    "x = np.arange(len(metrics))  # label locations\n",
    "bar_width = 0.35  # width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plotting bars for model metrics\n",
    "ax.bar(x, [model_metrics[metric] for metric in metrics], width=bar_width, color='lightblue', label='Model Metrics')\n",
    "\n",
    "# Calculate means and error margins for industry standards\n",
    "industry_means = [(industry_ranges[metric][0] + industry_ranges[metric][1]) / 2 for metric in metrics]\n",
    "industry_errors = [(industry_ranges[metric][1] - industry_ranges[metric][0]) / 2 for metric in metrics]\n",
    "\n",
    "# Adding error bars to indicate the range of industry standards\n",
    "ax.errorbar(x + bar_width / 2, industry_means, yerr=industry_errors, fmt='o', color='red', capsize=5, label='Industry Range')\n",
    "\n",
    "# Adding labels, title, and custom x-axis tick labels\n",
    "ax.set_ylabel('Values')\n",
    "ax.set_title('Model Metrics vs Industry Ranges')\n",
    "ax.set_xticks(x + bar_width / 2)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_ylim(0, 1)  # Setting the y-limit to encompass typical ranges for these metrics\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# After all the evaluations and plots, print the single run evaluation metrics\n",
    "model_metrics = compute_model_metrics(predictions)\n",
    "print(\"Single Evaluation Metrics:\")\n",
    "print(model_metrics)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e627b9ca-789d-4d6c-a38b-abda109d4b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6d9819e-322a-45b2-a80d-5590a4250a65",
   "metadata": {},
   "source": [
    "## Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a3f295-29cb-48de-a8b9-8ff2baac1538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc062c20-765c-4e82-a848-2a8805901654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c263c19-7e78-46a3-9a2d-af527573de41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec514876-56ff-41d4-bace-3ed90dcc3851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7024c670-20e1-458a-b1ec-69bb00a68ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
